\documentclass[11pt,fleqn,openany]{book}
\usepackage[utf8]{inputenc}
\usepackage[top=3cm,bottom=3cm,left=3.2cm,right=3.2cm,headsep=10pt,letterpaper]{geometry}
\usepackage{xcolor}
\definecolor{ocre}{RGB}{38,131,255}
\definecolor{ForestGreen}{RGB}{34,139,34}
\usepackage{avant}
\usepackage{mathptmx}
\usepackage{microtype}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}

\usepackage[hidelinks]{hyperref} \def\UrlBreaks{\do\/\do-} % breaks long url in
% references
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{listings-golang}
\usepackage{color}
\usepackage{tikz}
\usepackage[labelfont=it,textfont={it},singlelinecheck=on,justification=centering]{caption}
\usepackage{amsmath}
\usepackage{float}
\usepackage{comment}
\usepackage{cite}

\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}
\lstset{numbers=left,numberstyle=\tiny, stepnumber=1, numbersep=5pt}
\lstset{language=TeX}
\setlength{\parskip}{1em}

\renewcommand{\baselinestretch}{1.2}

\newcommand{\x}[1]{{\tt #1}} \newcommand{\code}[1]{{\tt #1}}
\newcommand{\todo}[1]{{\color{red} TODO #1 }}
\newcommand{\bs}[1]{{\color{red}#1}}

\lstset{ % add your own preferences
    basicstyle=\footnotesize\tt,
    keywordstyle=\color{blue},
    numbers=left,
    numbersep=5pt,
    showstringspaces=false,
    stringstyle=\color{red},
    commentstyle=\color{ForestGreen},
    tabsize=2,
    language=Golang
}

\title{\textbf{\sffamily\color{white} \
Storj: A Decentralized Cloud Storage Network Framework}}
\author{\small\sffamily\color{white}
Storj Labs, Inc.}
\date{\small\sffamily\color{white} v3.0, August 23, 2018\\
\small\color{white}\url{https://github.com/storj/whitepaper}
}

\input{structure}

\begin{document}

\thispagestyle{fancy}
\chapterimage{images/header.eps}
\AddToShipoutPicture*{\put(0,0){\includegraphics[width=\paperwidth]{images/front.eps}}}

\maketitle

\begin{tikzpicture}[remember picture,overlay]
\node[rotate=45,scale=15,text opacity=0.1]
at (current page.center) {DRAFT};
\end{tikzpicture}

\todo{
Remaining big todo items:
\begin{itemize}
\item Datascience: write an appendix about repair bandwidth based on node
  churn and uptime
\item Datascience: Write an appendix about how we select RS numbers
\item Eng/marketing: Quality control/branding section - talk about how we
  plan to ensure satellite quality by quality control evaluations, formal
  partnerships, relationships, and not letting poor quality satellites
  use the brand.
\item i-Confluence
\item tie framework back to design constraints
\item tie concrete implementation back to framework
\end{itemize}

Remaining medium todo items:
\begin{itemize}
\item Anyone:
  Clean up distributed consensus positioning in the Metadata section.
\item Eng: Describe how path encryption works
\item Eng: Describe payer (satellite) ids and why they're needed
\item Eng: General data repair framework description
\item Eng: more macaroon details
\item Eng: future work section about hot files
\item get a better latex theme
\end{itemize}

Remaining little todo items:
\begin{itemize}
\item Eng: Write a future work section about eliminating TLS handshakes
\item Eng: Write about how piece ids are generated
\item Eng: Future work section about reputation sharing
\item Eng: Future work section about finding a better data structure than
 a bloom filter for gc
\item Figure out latex theme licensing
\end{itemize}
}

\newpage

\tableofcontents\newpage

\section{Abstract}

Decentralized Cloud Storage represents a fundamental shift in
the efficiency and economics of large-scale storage.
Eliminating central control allows users to store and share data
without reliance on a third-party storage provider. Decentralization mitigates
many traditional data failures and outages while simultaneously increasing
security and privacy.
Decentralization also allows market forces to optimize on cheaper ways to
provide storage at a greater rate than any single entity could afford.
While there are many ways to build such a system, there are some specific
responsibilities any given implementation should address.
Based on our experience with petabyte-scale
storage systems, we introduce a modular framework for considering these
responsibilities and building our distributed storage network.
Additionally, we describe an initial
concrete implementation for the entire framework.

\section{Contributors}

This paper took the combined contributions of many individuals. Contributors
at Storj Labs, Inc. include but are not limited to:
Alex Bender,
Alex Leitner,
Benjamin Sirb,
Brandon Iglesias,
Bryan White,
Cameron Ayer,
Dan Sorenson,
Dennis Coyle,
Dylan Lott,
Garrett Ransom,
Helene Unland,
James Hagans,
Jennifer Johnson,
Jens Heimb√ºrge,
John Gleeson,
Jon Sanderson,
JT Olio,
Kaloyan Raev,
Kishore Aligeti,
Matt Robinson,
Moby von Briesen,
Nadine Farah,
Natalie Villasana,
Patrick Gerbes,
Philip Hutchins,
and Shawn Wilkinson.

We'd like to especially thank the other authors of the previous Storj v2 paper:
Braydon Fuller,
Chris Pollard,
Gordon Hall,
James Prestwich,
Josh Brandoff,
and Tome Boshevski.

We'd like to extend a huge thank you to everyone we talked to during the
design and architecture of this system for your valuable thoughts, feedback,
input, and suggestions.

\chapter{Introduction}

\todo{
Outline of this intro (and the entire paper) should be:
\begin{itemize}
\item Decentralized storage solves some big problems.
\item Decentralized storage addresses a specific market we're interested in.
\item To best address this market, our product must satisfy specific constraints.
\item We believe a specific framework is required to best satisfy these
  constraints. We will not need a v4 as this framework is fairly fundamental
  to our space.
\item Our concrete implementation is just one potential solution within our
  framework. We plan to improve our implementation within the framework over
  time.
\item We have made a product out of these concrete framework choices.
\end{itemize}
}

Decentralized storage has emerged as a potential solution to the challenge of
providing an economical cloud storage solution. It can address the rapidly
expanding amount of data for which current solutions are cost-prohibitive.
With an anticipated 44 zettabytes of data expected to exist by 2020 and a
market that will grow to \$92 billion USD in the same time frame, we are
convinced that decentralized cloud storage has the potential to address several
key segments within that market today, particularly related to long term
archival storage. \todo{citations for stats}
As the capabilities within decentralized storage mature, decentralized cloud
storage will be able to address a much wider range of use cases from basic
object storage to content delivery networks (CDN).

There are a handful of storage-based companies emerging that aim to create vast
networks to store data.
Storage products such as Wuala, Allmydata, Tahoe-LAFS, Space Monkey, Sia,
Maidsafe, Filecoin, Crashplan, Mozy, Hadoop Distributed File System (HDFS),
Amazon S3, Google File System (GFS), and others believe that a single computer
is not as powerful or secure as these networks.
While all the above mentioned companies share a common goal of building robust
and secure networks, they also share common principles of how this can be
achieved.
Companies generate redundancy for data in case of system failures, store this
redundancy in locations with varying degrees of failure isolation, and then
keep track of where the data was placed.
However, the technical implementations, designs, and other considerations
vary greatly with each organization.

Decentralized cloud storage is still rapidly advancing through the maturity
cycle to address these challenges, but its evolution
is subject to a specific set of design constraints which
define the overall requirements and implementation of
the network software components. When designing a distributed storage system,
there are many parameters to be optimized: speed, capacity, trustlessness,
byzantine fault tolerance, cost, etc.
But unlike optimizing cost or speed, data needs to be persistent throughout
the network with no loss.
One way to achieve this goal is to have nodes communicate with each other -
creating functional systems that are seamlessly integrated to store
and retrieve data.

We propose a framework that scales horizontally serving hundreds of millions of
people across the globe. Our system--the Storj Network--is a robust object store
that encrypts, shards, and distributes data to our global community for storage.
Data is served with low latency in a manner purposefully designed to prevent
breaches that have continuously plagued many companies.
In order to accomplish this task, we've designed our system to be modular,
consisting of independent components with task-specific jobs.
We've integrated these components with each other to implement a secure,
robust, and reliable storage system.

We organized the rest of this paper into five
sections. Section \ref{sec:design_constraints} discusses the design space
in which Storj operates and elaborates on specific design constraints which
we are focusing our optimization efforts on.
Section \ref{sec:framework} covers our framework, while \ref{sec:concrete}
proposes a simple concrete implementation of each component.
Section \ref{sec:product_details} covers specific details
about how we will deliver this implementation to users. Finally, Section
\ref{sec:future_work} covers future areas of research.

\chapter{Storj design constraints}\label{sec:design_constraints}

Before designing a system, it's important to first define its requirements.
There are many different ways to design a decentralized storage system, but
with the addition of a few requirements, the potential design space shrinks
significantly.
In our case, our design constraints are heavily informed by our product and
market fit goals.
By carefully considering each requirement, we can make sure that the framework
we choose is as universal as possible given its constraints.

\section{S3 compatibility}

At the time of publishing this paper, the flagship cloud storage product is
Amazon's Simple Storage Service, or S3 for
short. Most other currently available cloud storage products provide some form
of compatibility with the S3 API.

Our objective is for Storj to compete successfully in the wider cloud storage
industry and bring decentralized cloud storage into the mainstream, enabling
more people to enjoy greater security with less centralized control.
Until a decentralized cloud storage protocol becomes the {\em lingua franca} of
storage protocols, we should create a graceful transition path from centralized
providers for our users.
This will alleviate any switching costs for users with data currently stored on
a centralized provider.
To achieve this, the Storj implementation should allow
applications previously built against S3 be adjusted to work with Storj with
minimal friction or changes.
This adds strong requirements on feature set, performance, and durability.

At bare minimum, this requires methods such as those roughly sketched out in
Figure \ref{fig:s3-api-code}.

\begin{figure}
\begin{lstlisting}
  // Bucket operations
	CreateBucket(bucketName string)
	DeleteBucket(bucketName string)
	ListBuckets() []BucketInfo

  // Object operations
	GetObject(bucketName, objectPath string, offset, length int64) (
	    ObjectData, ObjectInfo)
	PutObject(bucketName, objectPath string, data ObjectData,
	    metadata map[string]string) ObjectInfo
	DeleteObject(bucketName, objectPath string)
	ListObjects(bucketName, prefix, startKey string,
	    limit int, recursive bool) ([]ObjectInfo, more bool)
\end{lstlisting}
\caption{Minimum S3 API}
\label{fig:s3-api-code}
\end{figure}

\section{Device failure and churn}

For all storage systems, but especially distributed ones, component
failure is a guarantee. All hard drives fail after enough wear
\cite{backblaze-hd-2018-q1} and servers providing network access to
these hard drives will eventually fail, too. Network links may die, power
failures could cause havoc sporadically,
and storage media become unreliable with time.
For data to outlast individual component
failures, data must be stored with enough redundancy to recover from these
failure scenarios.
Perhaps more importantly, no data should be assumed to be stationary as all
data must eventually be moved. In such an environment, redundancy, data
maintenance, repair, and replacement of lost redundancy must be considered
facts of life, and the system must account for these issues.

Furthermore, decentralized systems are susceptible to high churn rates, where
participants join the network and then leave for various reasons, well
before their hardware has actually failed. A network with a high churn rate will
use large amounts of bandwidth just to ensure durability of the data and such
a network will fail to scale. As a result, a scalable, highly durable storage
system must give preference to stable nodes and endeavor to keep the churn rate
as low as possible.

Churn rates affect the network much more than hardware failure. As an
illustration, Maymounkov {\em et al.} found \cite{kad} that in
decentralized systems, the probability of a node staying connected to the
network for an additional hour is an {\em increasing}
function of uptime (figure \ref{fig:kad-uptime}).
In other words, the longer a node is a
participant in the network, the more likely it is to continue
participating. This gives our system a strong incentive to prefer long-lived,
stable nodes to minimize churn.

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{images/uptime.png}
\caption{Probability of remaining online another hour as a function of uptime.
The $x$ axis represents minutes. The $y$ axis shows the fraction of nodes
that stayed online at least $x$ minutes that also stayed online at least
$x+60$ minutes. Source: Maymounkov {\em et al.} \cite{kad}}
\label{fig:kad-uptime}
\end{figure}

See Appendix \ref{appendix:RS} for a discussion about how repair bandwidth
varies as a function of node churn.

\section{Latency}

A decentralized, distributed storage system has the potential to capitalize on
massive opportunities for parallelism in transfer rates, processing,
and a number of other factors. Parallelism by
itself is a great way to increase overall throughput even when individual
network links are slow. However, parallelism cannot by itself improve {\em
latency}. If an individual network link has fixed latency and is a required part
of an operation, the latency of the required network link will be the lower
bound for the overall operation.
Therefore, a distributed system
intended for high performance applications must aggressively optimize for low
latency, both at the individual process scale and at the overall architecture
scale.

\section{Bandwidth}

Global bandwidth availability is increasing year over year; however, access to
high-bandwidth internet connections is unevenly distributed in different
parts of the world.
While users in some
countries can easily access symmetric, high-speed, unlimited bandwidth, users in
other countries may have significant difficulty in obtaining access to the same.

In the United States, the way many residential internet service providers (ISPs)
operate presents two specific challenges for designers of a
decentralized network protocol. The first challenge is that the internet
connection is often asymmetric. Customers subscribe to internet service
based on an advertised download speed, but the upload speed is potentially an
order of magnitude or two slower. The second challenge is that bandwidth is
sometimes "capped" by the ISP at a fixed amount of traffic per month.
For example, in many
US markets, Comcast imposes a one terabyte per month bandwidth cap with stiff
fines for customers who go over this limit.
An internet connection with a cap of 1 TB/month cannot average more than
385 KB/s over the month without exceeding the monthly bandwidth cap, even if
the internet connection advertises speeds of 10 MB/s or higher.
Such caps impose
significant limitations on the bandwidth available at any given moment.

With device failure and churn guaranteed, any decentralized system will have a
corresponding amount of repair traffic. It is therefore important to make sure
there is enough headroom for the bandwidth required for data maintenance, over
and above that required for data storage and retrieval. Designing a
storage system that is careless with bandwidth usage would be to give undue
preference to storage providers with access to unlimited high-speed bandwidth,
recentralizing the system to some degree. In order to keep the storage
system as decentralized as possible while working in as many environments
as possible, bandwidth usage must be aggressively minimized.

Please see Appendix \todo{} for a discussion on how available bandwidth
combined with required repair traffic limits usable space.

\section{Security and privacy}

One of the primary focuses of the Storj network is to ensure that its users'
data privacy is protected. This is not addressed as an afterthought but is
built into the design of the clients, network, and other related services.

Any object storage platform in our market should ensure both the privacy and
security of data stored; however,
decentralized storage platforms include an additional layer of
complexity and risk associated with the storage of data on inherently
untrusted nodes. Because decentralized storage platforms cannot take many
of the same shortcuts datacenter-based approaches can (e.g. firewalls, DMZs,
etc.), decentralized storage must be designed from the ground up to support
enhanced security and privacy at all levels of the system.

Data security encompasses a wide range of concerns including
identity and access management, prevention of tampering or unauthorized
modification, prevention of ransomware attacks, system vulnerabilities, and
malicious insiders. Data privacy, on the other hand, includes storage of data
without providing access to the underlying data and its metadata by
Storj Labs, third-party operators of system components, or malicious actors.

Certain categories of data are subject to specific regulatory compliance.
In addition to a baseline level of security and privacy concerns,
cloud storage should also address issues of regulatory compliance that
specify conditions for the location, governance, or control of data such as
HIPAA or GDPR.

One last constraint is that our solution be open source. Open source software
provides the level of transparency and assurance needed to prove that the
behaviors of the system are as advertised. Customers should be able to evaluate
that our solution is programmatically resistant to established
attack vectors, manipulation, and censorship, in addition to having a lack of a
centralized point of attack or failure.

\section{Object size}

Large storage systems can broadly be classified into two groups by average
object size. When storing a large amount of small pieces of information, a
database is generally the preferred solution.
On the other hand, when storing many large
files, an object store or filesystem is ideal. We classify a "large" file as a
few megabytes or greater in size.

The initial product offering by Storj Labs is designed to function primarily as
an object store for larger files. While future improvements may enable
database-like use cases, the predominant use case described in this paper is
object storage. Protocol design decisions are made with the assumption that the
vast majority of objects stored will be a couple of megabytes or larger.

It is worth pointing out that this will not negatively impact use cases that
require reading lots of files smaller than a megabyte. Such cases can be
addressed with a packing strategy, where many small files are aggregated and
stored together as one large file.
As the protocol supports seeking and streaming, small files can even be
downloaded without requiring full retrieval of the aggregated object they were
packed into.

\section{Decentralization}

A decentralized application in our view is a service that no single entity
operates. Further, no single entity should be solely responsible for the cost
associated with running the service.

Decentralization as an architectural and design constraint is driven in part
by the economic consideration of the cost to instantiate and manage
infrastructure or pay for utilities and bandwidth. We believe that there
are significant underutilized resources at the edge of the network and at
the margin for many smaller operators. We believe there is a long tail of
resources that are presently unused or underused that could provide affordable
but geographically distributed cloud storage. Perhaps some small operator
has access to cheaper electricity than standard data centers or another small
operator has access to cheaper cooling. Many of these small operator
environments are not substantial enough to run an entire datacenter-like
storage system, but we believe that in aggregate, enough small operator
environments exist that their combination over the internet constitute
significant opportunity and advantage for cheaper and faster storage.

Operating a decentralized network requires node operators to contribute
resources (e.g. network bandwidth, compute, storage, and energy) in exchange
for compensation (i.e. the opportunity to earn cryptocurrency).
In most instances, increase in demand for a decentralized service drives
increase in adoption and proliferation of storage nodes.
This effectively increases the available capacity while at the same time
driving costs down.
There is no need to build expensive data centers or pay for unused resources.

Our decentralization goals are also driven by our desire to detangle
fundamental infrastructure (storage) from the hands of a few major entities.
We believe that there exists inherent risk in trusting a single entity,
company, or organization with a significant fraction of the world's data.
Many possible unfavorable outcomes may result from trusting a single entity,
such as the company's roadmap changes and the product changes to be less
useful, the company's position on data collection changes and begins to sell
your metadata to advertisers, or perhaps the company goes out of business or
fails to keep your data safe. By creating an equivalent or better system,
many users worried about single-entity risk will have a viable alternative.

We adopt a decentralized architecture because on the whole, it addresses the
limitations faced by centralized cloud storage.
Decentralization results in a single globally distributed network that can
serve a wide range of storage use case from archival to CDN, whereas
centralized storage systems require different architectures, implementations
and infrastructure to address each of those same use cases.
With decentralized architecture, Storj could cease operating and the data
would continue to be available.

\section{Byzantine Fault Tolerance}

Unlike datacenter-based solutions like Amazon S3, Storj operates in an untrusted
environment, where individual storage providers are not necessarily assumed to be
trustworthy. Storj operates over the public internet, allowing anyone to sign
up to become a storage provider.

We adopt the BAR (Byzantine, Altruistic, Rational) model \cite{bar} to discuss
participants in the network.
{\em Byzantine} nodes may deviate arbitrarily from the suggested protocol for
any reason. Some examples include nodes that are broken or nodes that
are actively trying to sabotage the protocol. In general, a Byzantine node is
one that optimizes for a utility function that is independent of the one
given for the suggested protocol.
Inevitable hardware failures aside, {\em Altruistic} nodes
participate in a proposed protocol even if the rational choice is to deviate.
{\em Rational} nodes participate or deviate only when it
is in their net best interest to do so.

Some distributed storage systems (e.g. Amazon S3) operate in an environment
where all nodes are altruistic. Amazon owns all of their nodes directly and
rogue operations engineers are not generally coopting node behavior for their
own purposes.
On the other hand, Storj operates in an environment where every node is
managed by its own independent operator.
In this environment we can expect that a majority
of storage nodes are rational and a minority are byzantine. Storj assumes no
altruistic nodes. Any potential implementation must account for this distinction.

\section{Coordination contention}

\todo{explain why avoiding coordination is good, coordination contention
is bad, and reference calm conjecture
\cite{calm1, calm2}, i-confluence \cite{i-confluence}, hat \cite{hat}, and
anna \cite{anna}}

\section{Marketplace and economics}

Designing a decentralized storage network in pursuit of creating and
operating a business presents additional design constraints than when only
pursuing an ideological goal of a purely decentralized application
for the sake of decentralization.
As a business entity, Storj Labs develops software to aggregate and resell
unused storage space on devices owned and operated by unaffiliated third
parties. This impacts design in a number of ways.

First, it is important to separate of the function of storing data on
the network (demand) from the function of sharing space and bandwidth
on the network (supply). This is because
the overwhelming majority of potential partners and customers who will
pay to store data on the
network do not also have a corresponding supply of storage space to share.

Second, individuals providing the supply of unused storage must
be compensated fairly.
This includes aligning the economic incentives of the network to ensure
that the rational nodes on the network (the majority of operators) behave
as similarly as possible to the expected behavior of altrustic nodes.
Opportunities should likewise be eliminated that elicit any byzantine behavior.

Third, to drive network growth and adoption, we must have a price-competitive
product and attractive
opportunities available for demand-side partners. To create additional
incentives for partners or customers to bring data to the network,
the price charged for storage and bandwidth--combined with the other
benefits of decentralized storage--must be
more compelling and economically beneficial than alternative storage solutions
in the market.

Decentralized storage requires the processing of a large volume of
transactions. A marketplace consisting of potentially millions of
demand-side partners, customers, and end users storing billions of pieces of
encrypted and encoded files on tens of millions of storage nodes and devices
operated by millions of supply-side partners or individuals can drive a lot
of traffic.
The network must be able to accurately account for billions of storage and
bandwidth utilization transactions, then appropriately bill and pay for the
aggregated utilization of resources.

The network must be able to account for ensuring efficient and timely billing
and payment and regulatory compliance for tax and other reporting.
Decentralized application architectures have seen growing adoption of the
incorporation of a cryptocurrency for use in the transmission of value in
exchange for utilization of shared resources on the network.
In this case, Storj has implemented the STORJ utility token.
This presents the additional requirement that the network be able to process
an extremely large volume of cryptocurrency transactions.

Because Storj is open source, the network must account for the operation of
servers by unaffiliated third parties and must be able to coordinate payment
as well as aggregate reputation data among and between components of the
network.
Building a network of contributors will increase the ability to bring new
capabilities to market faster.

\todo{this section is a combination of a problem statement and a solution.
some of this should be put into the concrete implementation section}

As the network scales and unaffiliated third parties operate heavy clients,
Storj must be able to maintain a registry of reliable heavy clients and derive
value from the operation of those heavy clients.

Storj will need to differentiate the software from the network as well as the
supply side from the demand side. Storj will brand the two aspects separately:
\begin{description}
\item[Storj.io] - the brand for supply-side storage node operators contributing
storage and bandwidth to the network in exchange for compensation.
\item[Tardigrade.io] - the brand for demand-side partners and customers who
purchase storage and bandwidth on the network with the expectation of high
durability, resilience, reliability and backed by an industry-leading SLA.
\end{description}

Due to the global network of supply and demand for storage and bandwidth, the
network must be able to transact in virtually any form of value whether that it
be fiat currency, cryptocurrency or some other form of barter.

Finally, the Storj roadmap will be aligned to the economic drivers of the
network.
New feature additions and changes to the concrete implementations of framework
components will be driven by applicability to specific object storage use cases
and the relationship between features and performance to the price of storage
and bandwidth relative to those use cases.

\chapter{Framework}\label{sec:framework}

After having considered our design constraints, the next goal is to design
a framework consisting of relatively fundamental components.
The framework should describe
all of the components that must exist to satisfy our constraints.
As long as our design constraints remain constant, this framework will, as
much as is feasible, describe Storj now and Storj in 10 years from now.
While there will be significant design freedom within the framework,
this framework will obviate the need for future rearchitectures entirely, as
independent components will be able to replaced without affecting other
components.

\section{Framework overview}

At a high level, every design within our framework will do the following
things:

\begin{description}

\item[Store data] When data is stored with the network, a client will encrypt
it and break it up into multiple pieces. It will distribute the pieces
to peers in the network and generate and store some metadata about where to
find the data again.

\item[Retrieve data] When data is retrieved from the network,
the client will first recover the metadata about where to find the pieces.
Then the pieces will be retrieved and the original data will be reconstructed
on the client's local machine.

\item[Maintain data] Data is maintained in the network by replacing
missing pieces when the amount of redundancy drops below a certain threshold.
The data is reconstructed, and the missing pieces are regenerated and replaced.

\item[Pay for usage] A form of currency should be sent in exchange for
services rendered.

\end{description}

To make this framework feasible while satisfying our design constraints, we
will need to solve a number of complex challenges.
Inspired by Raft \cite{raft}, we
break up the design into a collection of relatively independent concerns and
then combine them to form the desired framework.

The individual components are:

\begin{enumerate}
\item Storage nodes
\item Peer-to-peer communication and discovery
\item Redundancy
\item Metadata
\item Encryption
\item Audits and reputation
\item Data repair
\item Payments
\end{enumerate}

\section{Storage nodes}

The most fundamental part of this network is the storage node.
The storage node's role is to store and return data.
Aside from reliably storing data, nodes should provide
network bandwidth and appropriate responsiveness.
Storage nodes are selected to store data based on various criteria: ping time,
latency, throughput, disk space, geographic location, uptime, history of
responding accurately to audits, etc.
In return for their service, nodes are rewarded for their participation via
payments.

Because storage
nodes are selected via changing variables external to the protocol, node
selection is an explicit, nondeterministic process in our framework. This means
that we must keep track of which nodes were selected for each upload via a
small amount of metadata; we can't select nodes for storing data implicitly or
deterministically as in a system like Dynamo \cite{dynamo}. This decision
implies the requirement of a separate metadata storage system to keep track
of selected nodes.

\section{Peer-to-peer communication and discovery}

All peers on the network communicate via a ubiquitous standard protocol. The
framework requires that this protocol:

\begin{itemize}
\item provides peer reachability, even in the face of firewalls
and NATs. This may require techniques like STUN, UPnP, NAT-PMP, etc.
\item provides authentication, where each participant knows
exactly the identity of the peer with whom they are speaking to avoid
man-in-the-middle attacks.
\item provides privacy, where only the two peers
know what transfers between them.
\end{itemize}

Additionally, the framework requires a way to look up peer network addresses
by node ID so that, given a peer's network address, any other peer can connect
to it. This responsibility is similar to the service the internet's standard
domain name system (DNS) provides, which is a mapping of an identifier to an
ephemeral connection address. To achieve this, a network overlay can be
built on top of our peer-to-peer communication protocol that provides this
functionality. See Section \todo{ref} for implementation details.

\section{Redundancy}

At any moment, any storage node could go offline permanently. Our redundancy
strategy must store data in a way that provides access to the data with high
probability, even though any given number of individual nodes may be offline. To
achieve a certain level of {\em durability} (the probability that data will
remain available in the face of failures), many products in this space use
simple replication. Unfortunately, this ties durability to the network {\em
expansion factor}, which is the storage overhead for reliably storing data.

For example, suppose a certain desired level of durability requires a
replication strategy that makes eight copies of the data. This yields an
expansion factor of 8x, or 800\%. This data then needs to be stored on the
network, using bandwidth in the process. Thus, more replication results in more
bandwidth usage for a fixed amount of data. As discussed in the protocol design
constraints, high bandwidth usage prevents scaling, so this is an undesirable
strategy for ensuring a high degree of file durability.

Instead, {\em erasure
codes} are a more general and flexible scheme for manipulating data
durability without tying it to bandwidth usage. Importantly, erasure codes allow
changes in durability without changes in expansion factor!

\todo{fix flow:

An erasure code is often described by two numbers, $k$ and $n$. If a block of
data is encoded with a $(k,n)$ erasure code, there are $n$ total generated {\em
erasure shares}, where only any $k$ of them are required to recover the original
block of data. If a block of data is $s$ bytes, each of the $n$ erasure shares
is roughly $s/k$ bytes. Besides the case when $k=1$ (replication), all erasure
shares are unique. Interestingly, the durability of a $(k=20,n=40)$ erasure code
is better than a $(k=10,n=20)$ erasure code, even though the expansion factor
(2x) is the same for both! Intuitively, this is because the risk is spread
across more nodes in the $(k=20,n=40)$ case. These considerations make erasure
codes an important part of our general framework.

With the simplifying assumption that $p$ is the monthly node
birth/death \todo{pick one} rate (the proportion that describes the number of
nodes
leaving/joining the network per month), we can model file durability
as the CDF of the Poisson distribution with mean $\lambda=pn$,
where we expect $\lambda$ pieces of the file to be lost monthly. To estimate
durability, we consider the CDF up to $n-k$,
looking at the probability that at most $n-k$ pieces
of the file are lost in a month and the file can still be rebuilt.
The CDF is given by:
\begin{equation}
P(D) = e^{-\lambda} \sum_{i=0}^{n-k} \frac{\lambda^i}{i!}.
\label{eq:poiss_cdf}
\end{equation}
By being able to tweak the durability independently of the expansion factor,
very high durabilities can be achieved with surprisingly low expansion factors.
Because of how limited bandwidth is as a resource, eliminating replication as a
strategy entirely and using erasure codes only for redundancy causes a drastic
decrease in bandwidth footprint.
It further causes a significant increase in the funds available per byte to
storage nodes due to the decreased dilution of incoming funds to storage node
payment relative to larger expansion factors.

\begin{table}[h]
\centering
\begin{tabular}{c c c l}
$k$ & $n$ & Exp. factor & P(D) \\
\hline 2 & 4 & 2 & 99.207366813274616391\%\\
4 & 8 & 2 & 99.858868985411326445\%\\
8 & 16 & 2 & 99.995462406878260756\%\\
16 & 32 & 2 & 99.999994620652776179\%\\
32 & 64 & 2 & 99.999999999990544376\%\\
\end{tabular}
\caption{$P(D)$ for various choices of $k$ and $n$, assuming $p=0.1$.}
\end{table}

\begin{figure} \centering
\todo{fix graphic title to make more english sense}
\includegraphics[width=\linewidth]{durability/durability.eps}
\label{fig:durability}
\end{figure}

}

\subsection{Erasure codes' effect on streaming}

Erasure codes are used in many streaming contexts such as audio CDs and
satellite communications, so it's important to point out that using erasure
coding in general does not make our streaming design requirement more
challenging. Whatever erasure code is chosen for our framework, streaming can be
added on top by encoding small portions at a time, instead of attempting to
encode a file all at once. See the structured file storage section for more
details. \bs{add reference to the section}

\subsection{Erasure codes' effect on Long tails}

Erasure codes enable an enormous performance benefit, which is the ability to
avoid waiting for "long-tail" response times \cite{tail-at-scale}. For uploads,
a file can be encoded to a higher $(k, n)$ ratio than necessary for desired
durability guarantees.
During an upload, after enough pieces have uploaded to gain required
redundancy, the remaining additional uploads can be cancelled, allowing the
upload to continue as fast as the fastest nodes in a set, instead of waiting
for the slowest nodes.
Downloads are similarly improved. Since more redundancy exists
than is needed, downloads can be served from the fastest peers, eliminating a
wait for temporarily slow or offline peers.

The goal is a protocol that allows for every request to be satisfiable by the
fastest nodes participating in any given transaction, without needing to wait
for a slower subset.
Focusing on operations where the result is only dependent on the fastest
nodes turns what could be a potential liability (highly variable performance
from individual actors) into a great source of strength for a distributed
storage network.

\section{Metadata}

Once we split an object up and selected storage nodes on which to store the
new pieces, we now must keep track of which storage nodes we selected.
Moreover, to maintain S3 compatibility, the user must be able to choose an
arbitrary key, often treated like a path, to identify this mapping of data
pieces to node. This implies the necessity of a metadata storage system.

S3 compatibility once again imposes some tight requirements.
We should support
hierarchical objects (paths with prefixes), per-object key/value storage,
arbitrarily large files, arbitrarily large amounts of files, and so on.
Metadata values
should be able to be stored, retrieved, and removed by arbitrary key, and
deterministic iteration over those keys will also be required. Every time an
object is added, edited, or removed, one or more entries in this metadata
storage system will need to be adjusted. As a result, there could be heavy churn
in this metadata system, and across the entire userbase the metadata itself
could end up being sizeable.

To provide some examples, suppose in
a few years this system stores 1 total exabyte of data, where the average object
size is 50MB and our erasure code is such that $n=40$. This metadata will need
to keep track of which 40 nodes were selected for each object.
1 exabyte of 50MB objects is 20 billion objects. If
each metadata element is roughly 40*64+192 bytes (info for each selected node
plus the path and some general overhead), there are over 55 terabytes of
metadata of which to keep track.
Fortunately, this metadata can be heavily partitioned by user. A user storing a
100 terabytes of 50MB objects will only incur a metadata overhead of 5.5
gigabytes. It's worth pointing out that these numbers vary
heavily with average object size: the larger the object size, the less the
metadata overhead.

Aside from scale requirements, the desired API is straightforward and
simple: \x{Put} (store a pointer given a path), \x{Get} (retrieve a pointer
given a path),
\x{List} (paginated, deterministic listing of existing paths), and \x{Delete}
(remove a path).

One of our framework's focuses is enabling this component -- metadata
storage -- to be interchangeable per user. Specifically, we expect to ship with
multiple implementations of metadata storage that users will be allowed to
choose between. \todo{reference i-confluence and coordination minimization}

\section{Encryption}

Regardless of storage system, our design constraints require total security
and privacy, so any such metadata will be encrypted.
Data should be encrypted as early as possible in the data storage pipeline,
ideally before the data ever leaves the source computer. This means that an
S3-compatible interface or appropriate similar client library should run
colocated on the same computer as the user's application.

Encryption should use a pluggable mechanism that allows users to choose their
desired encryption scheme as well as store metadata about that encryption
scheme to allow them to recover their data using the appropriate decryption
mechanism.

To support rich access management features, the same encryption key should not
be used for every file, as having access to one file would result in access
to decryption keys for all files. Instead, each file should be encrypted with
a unique key, such that users can share access to certain selected files
without giving up encryption details for others.

Because each file should be encrypted differently with different keys and
potentially different algorithms, the metadata about that encryption must
be stored somewhere in a way that is secure and reliable. This metadata will
be stored in the previously discussed metadata storage system, itself encrypted
by a deterministic, hierarchical encryption scheme.
A hierarchical encryption scheme similar to
BIP32 \cite{bip32} will allow subtrees to be shared without sharing their
parents, and will allow some files to be shared without sharing other files.

Like all other metadata, paths themselves can be encrypted using a hierarchical
encryption scheme. \todo{ref to hd scheme section}

\section{Audits and Reputation}

Incentivizing storage nodes to accurately store data is of paramount importance
to
the viability of this whole system. As such, it is important to be able to
validate and verify that storage nodes are accurately storing what they have
been
asked to store.

Many storage systems use audits as a way of determining when to do repair and
which files to repair. Our storage system does not.
Instead, we are extending the probabilistic nature of
common per-file {\em proofs of retrievability} \cite{proof-of-retrievability}
to range across all files.

In our storage system,
audits are simply a mechanism by which a node's degree of stability is
determined. Failed audits will result in marking a storage node as bad, which
could result in shuffling data to new nodes and avoiding that node altogether
in the future. File repair needs are detected via another mechanism.

Audits in this case are probabilistic challenges that confirm with a high
degree of certainty and a low amount of overhead that a storage node is well
behaved, is keeping the data it claims, and is not susceptible to hardware
failure or malintent. An audit functions as a spot check to help calculate a
storage node's future usefulness.

This partial auditing mechanism does not audit all bytes in all files and
leaves room for false positives, where the verifier believes the storage node
retains the intact data, when it has actually been modified or partially
deleted. Fortunately, the probability of a false positive on an individual
partial audit is easily calculable (see Section \todo{}). When applied
iteratively to a storage node as a whole, detection of unexpected behavior
becomes certain to within a known and modifiable error threshold.

\todo{discuss reputation}

\section{Data repair}

An ever-present risk in any distributed storage system is file loss. While there
are many potential causes for file loss, storage node churn is the leading
risk, as evidenced by the findings of Maymonkov et al. that expected node
availability is an increasing function of uptime \cite{kad}. Storage nodes
may go offline due to hardware failure, intermittent internet connectivity, or
operator choices.
Because audits are validating that conforming nodes store data correctly, all
that remains is to detect when a storage node goes offline or becomes bad and
repair at-risk data.

We're taking a huge shortcut with the assumption that
probabilistic audits are enough for us to estimate the likelihood that a node
will have the data it should have; we can use the audits
along with node uptime (which is much more efficient than audits)
to calculate when a file is at risk.
We {\em only} consider {\em node} availability and configured repair thresholds
when determining which {\em files} to repair.

There are many other ways data might get lost in the network besides node churn:
corruption, malicious behavior, bad hardware, software error, user space
reclaimation, etc., but these issues are less serious than full node
churn (power loss, internet connectivity intermittency,
software shutdown or removal).
Our spot-check based audits will incentivize storage nodes to reliabily store
data
while estimating the rate at which data is actually stored reliably.
Therefore, our repair system only seeks to solve the node churn problem, and
we expect to account for varying
amounts of other issues by configuring Reed-Solomon erasure code
parameters according to differing network conditions.

\section{Payments}

Payments in decentralized networks are a critical part of maintaining a healthy
ecosystem of both supply and demand. Of course, decentralized payment systems
are still in their infancy in a number of ways.

For our framework, to achieve low latency and high throughput, one must
avoid naively placing a blockchain-based solution in the storage hotpath.
By this we mean that an adequately performant storage system cannot afford to
wait for blockchain operations. When operations should be measured in
milliseconds, waiting for a cluster of nodes to probabilistically come to
agreement is a non-starter. \todo{i-confluence}

Our framework instead emphasizes game theoretic models to ensure
that participants in the network are properly incentivized to remain in the
network and behave rationally to get paid.
Many of our decisions are modeled after real-world financial relationships.
Payments will be transferred during
a background settlement process that well behaved participants in the network
will cooperate in. Storage nodes in our framework should limit their exposure
to untrusted payers until confidence is gained that those payers are likely
to pay their bills.

The Storj network is payment agnostic.
The protocol does not require a specific payment type.
The network assumes STORJ as the default payment medium, but many other payment
types could be implemented, including Bitcoin, Ether, ACH transfer, or physical
transfer of live goats.

\chapter{Concrete implementation}\label{sec:concrete}

The framework we've described above we believe to be relatively fundamental
given our design constraints. However, within the framework there remains a
significant amount of freedom in choosing how to implement each component.

In this section, we lay out our initial implementation strategy. We expect
the details contained within this section to change over time, but believe the
details outlined here are viable and support a working implementation of our
framework.

As with our previous network, we will publish changes to this concrete
architecture through our Storj Improvement Proposal process \cite{sips}.

\section{Definitions}

\begin{description}
\item[Client] A user that would like to upload or download data from the network.

\item[Peer class] A cohesive collection of network services and
  responsibilities. There are three different peer classes that represent
  services in our network: storage nodes, satellites, and uplinks. Peer classes
  tend to be run separately by different operators.

\item[Storage Node] This peer class participates in the DHT, stores data for
  others, and gets paid for storage and bandwidth (via a bandwidth allocation
  protocol).

\item[Uplink] This peer class represents any application or
  service that wants to store data. Applications can store data via the
  S3-compatible API, or through our libstorj C-bindings. This peer class
  is not expected to remain online like the other two classes and is otherwise
  relatively lightweight. This peer class performs encryption, erasure encoding,
  and coordinates between the other peer classes on behalf of the customer.

\item[Satellite] This peer class participates in the DHT, caches
  DHT lookups, stores per-object metadata, keeps storage node reputation, pays
  storage nodes, performs audits and repair, and manages authorization and user
  accounts. Any user can run their own satellite, but we expect many users
  will elect to avoid the operational complexity and create an account on
  another satellite hosted by a trusted party like a friend, group, or
  workplace.

\item[Bucket] A \x{bucket} is an unbounded but named
collection of \x{file}s identified by \x{path}s. Each \x{path} represents one
\x{file}, and every \x{file} has a unique \x{path}.

\item[Path] A \x{path} is a unique identifier for a \x{file} within a
\x{bucket}. A \x{path} is a string of UTF8 codepoints that begins with a forward
slash and ends with something besides a forward slash. More than one forward
slash (referred to as the \x{path separator}) separate \x{path components}.

An example path might be \code{/etc/hosts}, where the \x{path components} are
\code{etc} and \code{hosts}.

We encrypt \x{paths} before they ever leave the customer's application's
computer.

\item[File/Object] A \x{file} (or \x{object} or \x{stream}) is an ordered
collection of 0 or more \x{segment}s. \x{segment}s have a fixed maximum size,
so the more bytes the \x{file} represents through \x{segment}s, the more
\x{segment}s there are.

A \x{file} also support a limited amount of key/value user-defined fields to
support extended attributes.

Like \x{path}s, the data contained in a \x{file} is encrypted before it ever
leaves the client computer.

\item[Segment] A \x{segment} represents a single array of bytes, between 0 and a
user-configurable maximum \x{segment} size. Breaking large \x{file}s into
multiple \x{segment}s provides a number of security and scalability advantages.

\item[Inline Segment] An \x{inline segment} is a \x{segment} that is small
enough it makes sense to store it "inline" with the metadata that keeps track of
it, such as a \x{pointer}.

\item[Remote Segment] A \x{remote segment} is a larger \x{segment} that will be
encoded and distributed across the network. A \x{remote segment} is larger than
the metadata required to keep track of its book keeping.

\item[Stripe] A \x{stripe} is a further subdivision of a \x{segment}. A
\x{stripe} is a fixed amount of bytes that is used as an encryption and erasure
encoding boundary size. Erasure encoding happen on \x{stripe}s individually,
whereas encryption may happen on a small multiple of stripes at a time. All
\x{segments} are encrypted, but only \x{remote segments} are erasure encoded.
A \x{stripe} is the unit on which audits are performed.

\item[Erasure Share] When a \x{segment} is a \x{remote segment}, its \x{stripe}s
will get erasure encoded. When a \x{stripe} is erasure encoded, it generates
multiple pieces called \x{erasure share}s. Only a subset of the \x{erasure
share}s are needed to recover the original \x{stripe}, but each \x{erasure
share} has an index identifying which \x{erasure share} it is (e.g., the first,
the second, etc.).

\item[Piece] When a \x{remote segment}'s \x{stripe}s are erasure encoded into
\x{erasure share}s, the \x{erasure share}s for that \x{remote segment} with the
same index are concatenated together, and that concatenated group of \x{erasure
share}s is called a \x{piece}. If there are $n$ \x{erasure share}s after erasure
encoding a \x{stripe}, there are $n$ \x{piece}s after processing a \x{remote
segment}. The $i$th \x{piece} is the concatenation of all of the $i$th
\x{erasure shares} from that \x{segment}'s \x{stripe}s.

\item[Pointer] A \x{pointer} is a data structure that keeps track of which
\x{piece storage nodes} a \x{remote segment} was stored on, or the \x{inline
segment} data directly if applicable.
\end{description}

\section{Storage Node}

The main duty of a storage node is to reliably store and return data.
Storage node operators
are individuals or entities that have excess hard drive space and want to earn
compensation for lending their space to others. Storage node operators will
download,
install, and configure Storj software locally, with no account required
anywhere. Storage node operators will select what disk space and bandwidth usage
is allowed during configuration.
Storage nodes will advertise during DHT communications what hard drive space is
still
available, how much bandwidth is available, and what their desired STORJ token
wallet address is.

Because Storj is optimized for larger files, storage nodes have no reason to do
anything more complex than store \x{pieces} directly on disk. As a result,
unlike the previous release of Storj that used KFS \cite{storj-v2}, Storj no
longer has a restriction on the maximum amount of data a storage node can store.

Storage nodes also keep track of optional per-\x{piece} time-to-live, or TTL.
\x{Pieces} may be stored with a specific TTL expiry where data is expected to
be deleted after the expiration date. If no TTL was provided, data is expected
to be stored indefinitely. This means storage nodes have a database of
expiration
times and must occasionally clear out old data.

Storage nodes must additionally keep track of signed bandwidth allocations to
send to
satellites for later settlement and payment. This also requires a small
database. Both TTL and bandwidth allocations are stored in a SQLite
\cite{sqlite} database.

Storage nodes can choose which satellites to work with. If storage nodes work
with
multiple satellites (the default behavior), then payment may come from
multiple sources on varying payment schedules.
Storage nodes are paid by specific satellites for returning data when
requested in
the form of egress bandwidth payment. Bandwidth payment is made payable after
the storage node sends in signed bandwidth allocation messages.
Storage nodes are also paid for data at rest.
Storage nodes are expected to reliably store all data sent to them and are
paid
with the assumption that they are faithfully doing so.
Storage nodes that fail random audits will be removed from the pool and will
receive
limited to no future payments.
Storage nodes are {\em not} paid for the initial transfer of data to store
(ingress
bandwidth). This is to discourage storage nodes from deleting data only to be
paid for
storing more. Storage nodes are not paid for DHT or other maintenance traffic.

Storage nodes will support three methods: \code{get}, \code{put}, and
\code{delete}. They store {\em pieces}.
Each method will take a {\em piece ID}, a {\em payer ID} (the ID of the
associated Satellite instance) and signature, an optional TTL,
and the other metadata required by the bandwidth allocation protocol
\todo{clean up the list of arguments, add ref to bap}.

The \code{put} operation will take a stream of bytes and store the bytes such
that any subrange of bytes can be retrieved again via a \code{get} operation.
\code{get} operations are expected to work until the TTL expires (if a TTL was
provided), or until a \code{delete} operation is received, whichever comes
first.

The {\em payer ID} forms a namespace. An identical {\em piece ID} with a
different {\em payer ID} refers to a different {\em piece}.

Storage nodes should allow administrators to configure maximum allowed disk
space usage and maximum allowed bandwidth usage over the last rolling 30 days.
They should keep track of how much is remaining of both, and reject operations
that do not have a valid signature from the appropriate payer.

\section{Peer-to-peer communication}

Initially, we'll be using the gRPC \cite{grpc} protocol on top of of the
Transport Layer Security protocol (TLS) on top of the $\mu$TP
\cite{utp} transport protocol with added Session Traversal Utilities for NAT
(STUN) functionality. STUN provides NAT traversal, $\mu$TP provides reliable,
ordered delivery (like TCP would), TLS provides privacy and authentication,
and gRPC provides multiplexing and a convenient programmer interface.
Over time, we'll be replacing TLS to
reduce round trips due to connection handshakes in situations where the data is
already encrypted and forward secrecy isn't necessary. \todo{} See the Future
Work section for more details. Gateways will be provided in appropriate places
that allow for interoperability with web browsers.

Each node will operate its own certificate authority, which requires a
public/private keypair and a self-signed certificate. The certificate
authority's private key should ideally be kept in cold storage to prevent key
compromise.
It's important that the certificate authority private key be managed with good
operational security as key rotation for the certificate authority will require
a brand new node ID.

The {\em node ID} will be determined from the certificate authority
by hashing the DER-encoded public key.

As in S/Kademlia \cite{skad}, the {\em node ID} will be the hash of a public key
and will serve as a proof of work for joining the network. Unlike Bitcoin proof
of work \cite{bitcoin}, the work will be dependent on how many {\em trailing}
zero bits one can find in the hash output. This means that the node ID will
still be usable in a balanced Kademlia \cite{kad} tree.

Each node will also have revokable leaf key pair and certificate, signed by
the node's certificate authority. Nodes will use this leaf keypair for actual
communication. Each leaf will have a signed timestamp that Satellites should
keep track of per node. Should the leaf become compromised, the node can issue
a new leaf with a later timestamp. Interested peers should make note of newly
seen leaf timestamps and reject connections from nodes with earlier leaves.

When using TLS, every peer can ascertain the ID of the node with which it is
speaking by validating the certificate chain and hashing its peer's
certificate authority's public key. It can then be estimated how much work went
into constructing the node ID by considering the number of 0 bits at the end of
the ID. Satellites can configure a minimum proof of work required to pass an
audit, such that over time, the network will require greater proofs of work
due to natural user intervention. \todo{ref for future work}

For the few cases where a node cannot achieve a successful hole punch through a
NAT or firewall via STUN, uPnP, NATPmP, or a similar technique, manual
intervention and port forwarding will be required. In the future, nodes unable
to punch a port through their firewalls may rely on traffic proxying from
other, more available nodes, for a fee. See the bandwidth allocation protocol
\todo{ref} for a description of how fees work.

\section{Node discovery}

At this point, we have storage nodes and we have a means to communicate with
them if we know their address. We must account for the fact that storage nodes
will often be on consumer internet behind routers with constantly changing IP
addresses, so the overlay network's goal is to provide a means to look up a
node's latest address by node ID, somewhat similar to the role DNS provides
for the public internet.

The Kademlia DHT is a key/value store with a built-in node lookup protocol.
We utilize Kademlia as our primary source of truth for DNS-like
functionality for node lookup, while ignoring the key/value storage aspects of
Kademlia.
Only using Kademlia for node lookup eliminates the need for some other
features Kademlia would otherwise require, such as owner-based key
republishing, neighbor-based key republishing, store and retrieval of values,
etc. Furthermore, we avoid a number of known attacks by using the
S/Kademlia \cite{skad} extensions where appropriate.

Unfortunately, DHTs such as Kademlia require multiple network round trips for
many operations, which makes it difficult to achieve millisecond-level
response times. To solve this problem, we introduce a caching service on top
of Kademlia.

The caching service will attempt to talk to every storage node in the network
on an ongoing basis, perhaps once per hour. We expect this to scale for the
reasonable future, as ping operations are inexpensive, but admit a new solution
may be necessary \todo{future work}. The caching service will then cache
the last known good address for each node, evicting nodes that it hasn't talked
to after a certain period.
Fortunately, caching address information for an entire network of 80k nodes
(for example) can be done with 3MB of memory, so the space requirements are
negligible.

Based on this design, the cache should not be expected to be a primary source
of truth and results in the cache may be stale. Luckily, due to redundant
storage, the storage network will be resilient against an expected degree of
node churn, so the system will be robust even if some lookups in the the cache
fail or return incorrect addresses.
Further, because our peer-to-peer communication
system already provides peer authentication, an overlay network cache that
sometimes returns faulty
or perhaps deliberately misleading address lookup responses can only cause a
loss of performance, but not correctness.

We plan to host and help set up some well-known community-run overlay caches.
These caches will perform the duty of quickly returning address information
for a given node ID if the node has been online recently. Kademlia will be the
long-lived source of truth and can be used directly if correctness is valued
more highly than performance by certain customer classes.

\todo{talk about how we're using kademlia to advertise disk space and bandwidth
availability, and then storing that information in the cache}

\section{Redundancy}

We use the Reed-Solomon erasure code \cite{rs}. For each object that we store
we choose 4 numbers, $k$, $m$, $o$, and $n$, such that $k\le m\le o\le n$.
$k$ and $n$ are the standard Reed Solomon numbers, where $k$ is the minimum
required number of pieces for reconstruction, and $n$ is the total number of
pieces generated during creation.

$m$ and $o$ are the {\em minimum safe} and {\em optimal} values, respectively.
$m$ is chosen such that if the amount of available pieces falls below $m$, a
repair is triggered immediately in an attempt to make sure we always maintain
$k$ or more pieces. $o$ is chosen such that during uploads, as soon as $o$
pieces have finished uploading, remaining pieces up to $n$ are canceled as
described above. $o$ is chosen such that storing $o$ pieces is all that is
needed to achieve the desired durability goals; $n$ is thus chosen such that
storing $n$ pieces would be excess durability. $n-o$ is the amount of long
tail uploads we can tolerate and is thus the amount of slow nodes we are immune
to.

Our durability story does not end with our selection of these numbers.
Please see section \ref{sec:data_repair} for a discussion about how we repair
data as its durability drops over time.

See Appendix \ref{appendix:RS} for how we select our Reed-Solomon numbers.

\todo{future work: better erasure code algorithm than rs}

\section{Structured file storage}

\subsection{Files}

Many applications benefit from being able to keep metadata alongside files. For
example, NTFS supports "alternate data streams" for each file, HFS supports
resource forks, EXT4 supports "extended attributes," and more importantly for
our purposes, AWS S3 supports "object metadata" \cite{s3-object-meta}. Being
able to support arbitrarily named sets of keys/values dramatically improves
compatibility with other storage platforms. Every \x{file} will support a
limited set of arbitrary key/value metadata to support object metadata.

\subsection{Files as Segments}

\x{Files} should be designed both for small data and large data.
A \x{file} may be small enough it consists of only one segment.
If that \x{segment} is smaller than the metadata required to store it on the
network, the \x{segment} will be an \x{inline segment} and the data will be
stored directly inline with the metadata.

For larger \x{files}--\x{files} past a certain size--the data will be broken
into multiple large \x{remote segment}s. Segmenting in this manner has a
number of advantages to security, privacy, performance, and availability.

Maximum \x{segment} size is a configurable parameter. To preserve privacy, it is
recommended that \x{segment} sizes be standardized as a byte multiple, such as 8
or 32 MB. Smaller \x{segment}s may be padded with zeroes or random data.
Standardized sizes help frustrate attempts to determine the content of a given
\x{segment} and can help obscure the flow of data through the network.

Segmenting large files like video content and distributing the \x{segment}s
across the network separately reduces the impact of content delivery on any
given node.
Bandwidth demands are distributed more evenly across the network.
In addition, the end-user can take advantage of parallel transfer, similar to
BitTorrent or other peer-to-peer networks. Last, capping the size of segments
allows for more uniform storage node filling--a node must only have enough
space to store a segment to participate in the network, and clients don't have
to find nodes that have enough space for their specific large file.

\subsection{Segments as Stripes}

In many situations it's important to access just a portion of a piece of
data. Some large file formats such as large video files, disk images, or file
archives support the concept of seeking,
where only a partial subset of the data is needed for correct access or
read operation. To support these uses,
it's useful to be able to decode and decrypt only parts of a file.

A \x{stripe} should be no more than a couple of kilobytes, and erasure encoding
a single \x{stripe} at a time allows us to read portions of a large \x{segment}
without retrieving the entire \x{segment}, allows us to stream data into the
network without staging it beforehand, and enables a number of other useful
features.

\subsection{Stripes as Erasure Shares}

Erasure encoding gives us the chance to control network durability in the face
of unreliable \x{piece storage node}s. Erasure encoding schemes often are
described as $(k, n)$ schemes, where $k$ \x{erasure shares} are needed for
reconstruction out of $n$ total. For every \x{stripe}, $n$ \x{erasure share}s
are generated, where the network has an expansion factor of $n/k$.

For example, let's say a \x{stripe} is broken into 40 \x{erasure share}s
($n=40$), where any 20 ($k=20$) are needed to reconstruct the \x{stripe}. Each
of the 40 \x{erasure share}s will be $\frac{1}{20}$th the size of the original
\x{stripe}.

All $n$ \x{erasure share}s have a well defined index associated
with them. More specifically, for any given $n$, the $i$th share of an erasure
code will always be the same.

See section \todo{} for a breakdown of how varying the erasure code parameters
affects availability and redundancy.

\subsection{Erasure Shares as Pieces}

Because \x{stripe}s are already small, \x{erasure share}s are often much
smaller, and the metadata to keep track of all of them separately would be
immense relative to their size. Instead of keeping track of all of the
erasure shares separately, we pack all of the \x{erasure share}s together into
a few \x{piece}s.
In a $(k, n)$ scheme, there are $n$ \x{piece}s, where each
\x{piece} $i$ is the ordered concatenation of all of the \x{erasure share}s with
index $i$. As a result, where each \x{erasure share} is $1/k$th of a
\x{stripe}, each \x{piece} is $1/k$th of a \x{segment}, and only $k$
\x{piece}s are needed to recover the full \x{segment}.

\todo{piece ids are generated as the hmac of a root piece id and the storing
node id}

\subsection{Pointers}

The data owner will need knowledge of how a \x{remote segment} is broken up and
where in the network the \x{piece}s are located to recover it. This is contained
in the \x{pointer} data structure, and the owner can secure the \x{pointer} as
they wish.

\todo{code-based pointer definition. protobuf?}

\subsection{Paths}

\todo{}

\section{Metadata}

The most trivial implementation for the metadata storage functionality we
require would be to simply have each user use their preferred trusted database
such as PostgreSQL, SQLite, MongoDB, Cassandra\cite{cassandra},
Spanner\cite{spanner}, CockroachDB, to name a few. In many cases, this will
be acceptable for specific users, provided those users are managing appropriate
backups of their metadata. Indeed, the types of users who have petabytes of data
to store can most likely manage reliable backups of a single relational database
storing only metadata.

There are a few downsides with this punt-to-the-user approach, however, such as:
\begin{itemize}
\item {\bf Availability} - the availability of the user's data
is tied entirely to the availability of their metadata server. The counterpoint
here is that the availability can be made arbitrarily good with existing trusted
distributed solutions such as Cassandra, Spanner, or CockroachDB. Further, any
individual metadata service downtime does not affect the entire network. In
fact, the network as a whole can never go down.
\item {\bf Durability} -
if the metadata server suffers a catastrophic failure without backups, all of
the user's data will be lost. This is already true with encryption keys anyway,
but a punt-to-the-user solution increases the risk area from just encryption
keys considerably. Fortunately, the metadata itself can be periodically backed
up into the Storj network,
such that only needing to keep track of metadata-metadata
further decreases the amount of critical information that must be stored
elsewhere.
\item {\bf Trust} - the user has to trust the metadata server.
\end{itemize}

On the other hand, there are a few upsides:
\begin{itemize}
\item {\bf Use cases} - even in a catastrophic scenario, this design still
  covers all required use cases.
\item {\bf Control} - the user is in complete control of all of their data.
  There is still no organizational single point of failure. The user is free
  to choose whatever metadata store with whatever tradeoffs they like. Like
  Mastodon\cite{mastodon}, this solution is still decentralized. Further, in a
  catastrophic scenario, this design is no worse than most other technologies or
  techniques application developers frequently use (databases).
\item {\bf Simplicity} - other projects have spent multiple years on shaky
  implementations of byzantine fault tolerant consensus metadata storage.
  We can get a useful product to market without doing this work at all.
  This is a considerable advantage.
\end{itemize}

Our launch goal is to allow customers to store their metadata in a database of
their choosing. We expect and look forward to new systems and improvements
specifically in this component of our framework.

Please see Appendix \todo{} about why we've chosen to avoid trying to solve
the problem of Byzantine distributed consensus for now. See Future work
\todo{} for a discussion of medium to long term plans.

\section{Satellite}

As should be apparent, the data owner has to shoulder significant burdens
to maintain availability and integrity of data on the Storj network. Because
nodes cannot be trusted, data owners are responsible for selecting good
storage nodes, issuing and verifying audits, providing payments, managing file
state
and object metadata, etc. Many of these functions require high uptime and
significant infrastructure, especially for an active set of files. User run
applications, like a file syncing application, cannot be expected to efficiently
manage files on the network.

To enable simple access to the network from the widest possible array of client
applications, Storj implements a thin-client model that delegates trust to a
dedicated server that manages data ownership. The burdens of the
data owner can be split across the client and the server in a variety of ways.
This sort of dedicated server, called the satellite, has been developed and
released as Free Software. Any individual or organization can run their own
satellite to facilitate network access.

With respect to customer data, the satellite is designed to store only
metadata. It is never given data unencrypted and does not hold encryption keys.
The only knowledge of an object that the satellite is able to share with
third parties is metadata such as access patterns. This system protects the
client's privacy and gives the client complete control over access to the data,
while delegating the responsibility of keeping files available on the network
to the satellite.

In cases where the cost of delegating trust is not excessively high,
clients may use third-party satellites. Because satellites do not store
data and have no access to keys, this is still a large improvement over the
traditional data-center model. Many of the features satellites provide, like
storage node selection and reputation, leverage considerable network effects.
Data
sets grow more useful as they increase in size, indicating that there are
strong economic incentives to share infrastructure and information
in a satellite.

Applications using object stores delegate significant amounts of trust to the
storage providers. Providers may choose to operate public satellites as a
service.
Application developers then delegate trust to a specific satellite, as they
would to a traditional object store, but to a lesser degree. Future updates
will allow for various distributions of responsibilities (and thus levels of
trust) between customer applications and satellites. This shifts significant
operational burdens from the application developer to the service-provider.
This would also allow developers to pay for storage with standard payment
mechanisms, like credit cards, rather than managing a cryptocurrency wallet.
Storj Labs Inc. currently provides this service.

A specific satellite {\em instance} does not necessarily constitute one
server. A satellite may be run by a collection of servers and be backed by
a horizontally scalable trusted database for higher uptime.

The satellite is, at its core, one of the most complex and yet
straightforward components of our initial release that fulfills our framework.
Future framework-conforming releases nonwithstanding, the initial satellite
is a standard application server that wraps a trusted database such as
PostgreSQL, Cassandra, or something else. Users sign in to a specific
satellite with account credentials. The satellite is responsible for
keeping track of accounts and authorization, storage node contact information
and
reputation, and object metadata. The satellite is also responsible for
payments and data repair. Data available through one satellite instance is
not available through another satellite instance, though various levels of
export and import are planned.

The satellite is made up of components discussed earlier:

\begin{itemize}
\item A full DHT cache.
\item An account management and authorization system
\item A per-object metadata database indexed by encrypted path
\item A reputation and storage node statistics database
\item A storage node payment service
\item A data audit and data repair service
\end{itemize}

\section{Encryption}

Our encryption is authenticated encryption, with support for either the
AES-GCM cipher and the Salsa20 and Poly1305 combination NaCl calls "Secretbox"
\cite{nacl-crypto}. Authenticated encryption is used so that the user can know
if the data has been tampered with. Encryption keys are chosen by clients
randomly.

Data is encrypted in small batches of \x{stripes}, recommended to be 4KB or
less \cite{nacl-packetlen}. While the same encryption key is used for every
\x{stripe} in a \x{segment}, \x{segments} may have
different encryption keys. On the other hand, the nonce for each \x{stripe}
batch must be monotonically increasing from the previous batch throughout the
entire \x{stream}. The nonce wraps around to 0 if the counter reaches the
maximum representable nonce. The first nonce is chosen at random and is stored
with the \x{stream}'s metadata.
\todo{consider multipart upload}

Paths are also encrypted with authenticated encryption, but the nonce and key
must be deterministic, determined entirely from a root secret combined with the
unencrypted path. \todo{explain path encryption scheme}

This scheme protects the
content of the data from the \x{storage node} housing the data. The data owner
retains complete control over the encryption key, and thus over access to the
data.

\todo{describe path encryption}

Path encryption is optional, as encrypted paths make efficient sorted path
listing challenging. When path encryption is enabled (a per-bucket feature),
objects are sorted by their encrypted path name, which is relatively unhelpful
when interested in unencrypted paths. For this reason, users can opt in to
disabling path encryption. When path encryption is disabled, unencrypted paths
are only revealed to the user's chosen metadata storage system.

Order-preserving path encryption is left for future work.

\section{Authorization}

Encryption protects the privacy of data while allowing for the identification
of tampering, but authorization allows for the prevention of tampering by
disallowing clients. Users who are authorized should be able to add, remove,
and edit files, while users who are not authorized should not be able to do so.

First, metadata operations should be authorized. Users should authenticate with
their chosen metadata service, which should allow them given their authorization
configuration access to various operations.

Our initial metadata authorization scheme uses macaroons \cite{macaroons}.
Each account has a root macaroon and operations are validated against a supplied
macaroon's set of caveats.

\todo{discuss macaroon-based path restrictions}

Once authorized with a metadata service, that metadata service has an associated
{\em payer ID} \todo{discuss payer IDs} and is able to sign operations. All
operations with storage nodes require a specific payer ID and associated
signature. A storage node should reject operations not signed by the appropriate
payer ID. The client must retrieve valid signatures from the metadata service
prior to operations with storage nodes.

\section{Audits}

Some distributed storage systems (including the previous release of Storj
\cite{storj-v2}) discuss {\em Merkle tree proofs}, in which audit challenges
and expected responses are generated ahead of time as a form of compact
proof of retrievability \cite{proof-of-retrievability}. By using a Merkle tree
\cite{merkle-tree}, the amount of metadata needed to store these pre-generated
challenges and responses can be made to be negligible.

Unfortunately, in such a scheme, the challenges and responses must be
pre-generated, and without a periodic regeneration of these challenges, a
storage node can begin to pass most audits without storing all of the requested
data.

We do something else.
An assumption in our storage system is that most storage nodes are
reasonably well-behaved, and most data is stored faithfully. As long as that
assumption holds, Reed-Solomon is able to detect errors and even correct them,
via mechanisms such as the Berlekamp-Welch error correction algorithm \cite{bw}.
We are already using Reed-Solomon erasure coding
\cite{rs} on small ranges (\x{stripes}), so we use it to issue challenges and
verify responses as well.
This feature can be used for arbitrary audits without pregenerated challenges.

To perform an audit, we first choose a \x{stripe} to audit. We request that
\x{stripe}'s \x{erasure shares} from all storage nodes responsible. We then run
the Berlekamp-Welch algorithm \cite{bw} across all the \x{erasure shares}. When
enough storage nodes return correct information, any faulty or missing response
can easily be identified. These audit failures will be stored and saved in the
reputation system.

It is important that every storage node has a frequent set of random audits to
gain statistical power on how well-behaved that storage node is, but it is not
a requirement that audits are performed on every byte, or even on every file.
Additionally, it is important that every byte stored in the system has an equal
probability of being checked for a future audit to every other byte in the
system. Audits should happen uniformly at random by byte with replacement.

\section{Data repair}\label{sec:data_repair}

The node discovery system already has caches in place that have accurate and
up-to-date information about which storage nodes have been online recently.
When a storage node changes state from recently online to offline, this can
trigger a lookup in a reverse index in a user's metadata database, identifying
all \x{segment} \x{pointers} that were stored on that node.
For every \x{segment} that drops below the appropriate minimum safety
threshold, $m$, the segment should be downloaded and reconstructed, and the
missing pieces should be regenerated and uploaded to new nodes. Finally, the
\x{pointer} should be updated to include the new information.

As storage node nodes go offline--taking their file pieces with them--it will
be necessary for the missing pieces to be rebuilt once the entire file's pieces
fall below the predetermined threshold $m$. If a node goes offline, the
satellite will mark that nodes' file pieces as missing.
Once enough file pieces are lost, the satellite will download the
remaining file pieces from their corresponding storage nodes, using those
pieces
to rebuild the file's missing, encrypted, erasure encoded pieces.
Once the repair process is complete, the satellite will send the
recovered pieces to new storage nodes.

Users will choose their desired durability with their chosen metadata service
(which may impact price, among other things). This desired durability, along
with
statistics from ongoing audits, will directly inform what Reed-Solomon erasure
code choices should be made for new and repaired files, and what thresholds
should be set for when uploads are successful and when repair is needed. See
Appendix \todo{} for how we calculate these things given user inputs.

A practical upshot of this design is that for now, the satellite must
constantly stay running. If the user's satellite stops running, repairs will
stop, and data will eventually fall out of the network due to node churn. This
is similar to the design of how value storing and republishing works in
Kademlia \cite{kad}.

The ingress bandwidth demands of the audit and repair system are large, but the
egress demands are relatively small. A large amount of data comes in to the
system for audits and repairs, but just the formerly missing pieces get sent
back out.
While the repair and audit system can run anywhere, the bandwidth usage
asymmetry means that hosting providers that offer free ingress
make for an especially attractive hosting location for users of this system.
We will describe a Distributed Repair method in the Future Works section
\todo{ref} that
does not rely on the favorable pricing model of current hosting providers.

\subsection{Merkle trees}

Repairs are one of the few places latency doesn't matter. The data repair system
just needs to get through as many files as possible, but it doesn't matter if
a specific file takes longer. Throughput is much more important than
latency during repair. Furthermore, repair
is still a costly operation due to significant bandwidth and CPU usage
impacting a single operator, so repair work should be minimized.
As a result, when repairing a segment,
only the minimum number of pieces required should be downloaded.
Unfortunately, this means that
without full redundancy, erasure codes will be less effective at catching
errors. Further, the fallback safety mechanism that the user has for detecting
errors (authenticated encryption) is unavailable to the repair system (no
decryption keys).

Because full segments are repaired at a time, hashes of
each \x{piece} should be stored in the system via a Merkle tree
\cite{merkle-tree}, storing the root of the tree in the \x{pointer}. This allows
the repair system to correctly assess whether or not repair has completed
successfully without using extra redundancy for the same task.

A full copy of the leaves of the Merkle tree of \x{pieces} (enough to generate
the full tree) should be stored alongside each \x{piece} on each storage node,
with the root in the \x{pointer}, such that the only additional central
metadata storage required is just for the root.

Each repair should validate the tree before the \x{pointer} is updated to
point to new locations.

\section{Storage node reputation}

Reputation metrics on decentralized networks are a critical part of
enabling reasonable trust \todo{Can we eliminate the word trust here?}
between nodes
where there would otherwise be none. Reputation metrics
are used to ensure that bad actors
within the network are eliminated as participants, improving security,
reliability, and durability.

Storage node reputation can be divided into three subsystems. The first
subsystem is the initial vetting process, the second subsystem is a filtering
system, and the third system is a preference system.

The first subsystem slowly allows nodes to join the network.
When a storage node first joins the network, its reliability is unknown.
As a result, it will be placed into a vetting
process until enough data is known about it.
We propose the following way to gather data about new nodes
without compromising the integrity of the network.
Every time a file is uploaded, the system will select a small amount of
unvetted storage nodes to include in the list of target nodes.
The Reed-Solomon parameters will be chosen such that these unvetted storage
nodes will not affect the durability of the file, but will allow the network
to test the node
with a small fraction of data until we are sure the node is reliable.
After the storage node has successfully stored enough data for a long enough
period (potentially months),
the system will then start including that storage
node in the standard selection process used for general uploads.
Importantly, storage nodes get paid during this
vetting period, but don't receive as much data.

While new nodes require a proof of work to avoid some Sybil attacks
\cite{sybil-attack}, additional effort may be required to prevent
malicious and determined new nodes from overwhelming the vetting process and
preventing well-behaved new nodes from getting enough data to progress past it.
Satellite operators will be able to choose as a configuration
parameter the minimum proof of work required from storage nodes for new data.
Additionally, other schemes are possible, such as a form of proof of stake as
we proposed in our previous work \cite{sybil-cost}.

The filtering system is the second subsystem, and blocks bad storage nodes from
participating.
Certain actions a storage node can take are disqualifying events, and the
reputation system will be used to filter these nodes out from future uploads,
regardless of where the node is in the vetting process.
Actions that are disqualifying include failing too many audits,
failing to return data (with reasonable speed), and failing too many uptime
checks.
If a storage node is disqualified by failing too many audits, that node will no
longer be selected for future data storage and the data that node stores will
be moved to new storage nodes.
Likewise, if a client attempts to download a piece from a storage node that
the node should have and the node fails to return it, the
node will be disqualified. Importantly, storage nodes will be allowed to reject
and fail uploads without penalty, as nodes should be allowed to choose which
data to store and which satellite operators to work with.

It's worth reiterating that failing too many uptime checks is a disqualifying
event. Storage nodes can be taken down for maintenance, but if a storage node
is offline too much, it can have an adverse impact on the network. See Appendix
\todo{} for why uptime is so important in our storage system.

After a storage node is disqualified, the node must go back through the entire
vetting process again. If the node decides to start over with a brand new
identity, the node must restart the vetting process from the beginning (in
addition to generating a new node ID via the proof-of-work system). This
strongly disincentivizes storage nodes from being cavalier with their
reputation.

The third subsystem is a preference system. After disqualified storage nodes
have been eliminated, remaining statistics collected during audits
will be used to establish a preference for better storage nodes during uploads.
These statistics include performance characteristics such as throughput and
latency, history of reliability and uptime, geographic location, and other
desirable qualities.
They will be combined into a load-balancing selection process, such
that all uploads are sent to qualified nodes, with a higher likelihood of
uploads to preferred nodes, but with a non-zero chance for any qualified node.
Initially, we'll be load balancing with these preferences via a randomized
scheme such as the Power of Two Choices \cite{power-of-two-choices}, which
selects two options entirely at random, and then chooses the more qualified
between those two.

On the Storj network, preferential storage node reputation is only used to
select where new data should be stored, both during repair and during the
upload of new files, unlink disqualifying events.
If a storage node's preferential reputation decreases, its file pieces will not
be moved or repaired to other nodes.

There is no process planned in our system for storage nodes to contest their
reputation scores. It is in the best interest of storage nodes to have good
uptime, pass audits, and return data. Storage nodes that don't do these things
are not useful to the network. Storage nodes that are treated by satellites
unfairly should not accept future data from those payers. See the section
\todo{} about quality control on how we plan to ensure payers are incentivized
to treat storage nodes fairly.

Initially, storage node reputation will be individually determined by each
satellite. If a node is disqualified by one satellite, it could still
store data for other satellites. Reputation will not be shared between
satellites. Over time, as we plan to eliminate satellites,
reputation would then be determined globally.

\todo{future work section about reputation sharing}

\section{Payments}

In the Storj network, payments are made by uplink users who store data on the
platform to the satellite they utilize.
The satellite then pays storage nodes for the amount of storage and bandwidth
they
provide on the network.

Previous distributed systems have handled payments as hard-coded contracts.
For example, the previous Storj network utilized 90-day contracts to maintain
data on the network. After that period of time, the file would be deleted.
Other distributed storage platforms use 15-day renewable contracts that delete
data if the user does not login every 15 days. Others use 30-day contracts.
Moving forward, our network will not use contracts to manage payments and file
storage durations.

Satellites will pay storage nodes for the data they store long-term
and for object downloads.
Storage nodes will not be paid for the initial storage of data, but they
will be paid for storing the data month-by-month. At the end of the payment
period, a satellite will calculate earnings for each of its storage nodes.
Provided the storage node node hasn‚Äôt been blacklisted,
the storage node will be paid by the satellite for the data it has stored
over the course of
the month, per the satellite's records.

If a storage node misses a delete command due to the node being
offline, it will be storing more data than the satellite credits it for.
Storage nodes are not paid for storing such file pieces, but they
would eventually be cleaned up through the garbage collection process.
Because of the
way delete commands are issued, and because storage nodes are not expected to be
online at all times, storage nodes may be storing file pieces that were slated
for
deletion. This is factored into
the storage node payment amounts, meaning storage nodes are paid more than they
should for the file pieces they store, offsetting the lost revenue due to
storing garbage data.
This means that storage nodes who maintain higher availability
can maximize their profits by deleting files on request,
which minimizes the amount
of garbage data on their nodes.

The satellite maintains a database of all file pieces it is responsible for
and the storage nodes it believes are storing these pieces. Each day,
the satellite adds another day‚Äôs worth
of credits to each storage node for every file
piece
it should be storing. The satellite
also tracks file downloads in its database.
At the end of the month, each satellite
adds up all bandwidth and storage payments each storage node has earned and
makes
the payments to the appropriate storage nodes.

Satellites will track utilized bandwidth through a bandwidth allocation
protocol. To download a file, an uplink user connects to the satellite to
identify where its file pieces are stored and to provide a promise to pay for
the file download. The satellite sends a confirmation of this promise
to the uplink, along with file piece storage node node locations.
The console then sends the promise to pay directly
to the storage node nodes, along with the details on the file pieces it needs.
Each storage node then accepts or rejects this operation.
If a storage node accepts this
operation, it confirms and retains a copy of this promise to pay, sending the
client the file piece it needs. Later, the storage node sends the promise to
pay to
the satellite, and the satellite credits that storage node as having
successfully delivered the file piece.
\todo{ref to bandwidth allocation protocol section}

Satellites will also earn revenue from account holders for executing audits,
repairing files, and storing metadata. Every day, each satellite will execute
a number of audits across all of its storage nodes on the network. During an
audit,
if a storage node does not have the file it should be storing, it will be
immediately
blacklisted and the satellite will flag that storage node‚Äôs file pieces for
repair
in the system.
The satellite will be paid for both completing the audit
and for the repair,
once that file falls below the file piece threshold needed for
repair.

\todo{users pay satellites}
\todo{payers roll up payments every day, but pay every month}

\todo{Payment automation?}

\todo{Payment wallets vs payment addresses. }

\todo{}

See the payer reputation section for details on
how storage nodes will know to trust payers.

Payments to storage nodes will be calculated on a daily basis based on the
bandwidth
utilized and files stored, and will be paid at the end of each month.
If a storage node acts
maliciously and does not store files properly or maintain sufficient
availability, they will not be paid for the services rendered, and the funds
allocated to it will instead be used to repair any missing
file pieces and to pay new storage nodes for storing the data.

\subsection{Bandwidth allocation protocol}

A core component of our system requires knowing how much bandwidth is used
between two peers, so we introduce a protocol we call the Bandwidth Allocation
Protocol for correctly verifying that a certain amount of bandwidth was used
between two peers with incentives.
We don't measure all peer-to-peer traffic;
some operations are simply considered to be
the cost of doing business. This bandwidth traffic measurement only applies
to storage operations (storage and retrievals of pieces) and does not
apply to overlay traffic (Kademlia DHT) or other generic maintenance overhead.

\todo{diagram, gory detail, protobufs, update references}

When a client wants to perform an operation for $x$ bytes of bandwidth, it must
first get authorization from a satellite
that it has enough funds and is authorized to perform that operation.
The payer will return an {\em unrestricted
bandwidth allocation} message. This message will include the identity of the
payer, the identity of the client, an expiration timestamp, a serial number,
the maximum amount of bytes authorized, and the direction the bytes will flow
(whether or not the data will be transfered from or to the client).
The message will be signed by the payer.

%To use a metaphor, the payer is
%creating a blank check that is authorized up to $x$ in byte value and sending
%it to the client.

Once the client has an unrestricted bandwidth allocation, the client will then
create {\em restricted bandwidth allocations},
%or to continue the metaphor, a filled-in check,
indicating $y$ bytes have been transfered so far. The client
will start by sending a restricted allocation for some small amount,
perhaps only a few kilobytes,
so the storage node can verify the clients authorization.
If the allocation is signed correctly, the storage node will
transfer up to the amount listed in the restricted allocation ($y$ bytes) before
awaiting another allocation. The client will then send another allocation where
$y$ is larger, continuing to send allocations for data until $y$ has grown to
the full $x$ value.
For each transaction, the storage node only sends previously-unsent data,
so that the storage node only sends $y$ bytes total.

If the request is terminated at any time --
either planned or unexpectedly --
the storage node will keep the largest restricted bandwidth allocation it has
seen.
This largest restricted bandwidth allocation is the signed confirmation
by the client that the client agreed to bandwidth usage of up to $y$
bytes, along with the payer's confirmation of the client's bandwidth allowance.
The storage node will periodically send the largest restricted bandwidth
allocations it has received to appropriate satellites, at which point
satellites will pay the storage node for that bandwidth.

If the client can't afford the bandwidth usage, the satellite will not sign an
unrestricted bandwidth allocation, protecting the satellite's own reputation.
If the client tries to use more bandwidth than allocated,
the storage node will shut down the request.
The storage node can only get paid for the maximum amount a client has agreed
to,
as it otherwise has no valid bandwidth allocations to return for
payment.

\section{Satellite reputation}

Storage nodes have a strong incentive to avoid accepting data assigned to
satellites that don't have a good history of paying.

Initially, storage nodes will put satellites through a vetting process
where storage nodes limit their exposure to unknown satellites and build up
trust over time with specific payers that are likely to pay their bills.
Storage nodes
will have a configurable maximum amount of data that they will store for an
unknown satellite, and can use whether or not they get paid for that data
as input into
whether or not that satellite should be trusted for more data in the future.

Storage node operators will be able to opt in and out of working
with specific satellites they already trust or distrust.
Storj Labs will ship a list of recommended satellites that
they have already vetted for quality control that
node operators can elect to use.

If a satellite operator wants their
satellite included on the Storj-provided inclusion list, the satellite operator
may be required to pay Storj for insurance such that Storj can pay storage
nodes on the satellite operator's behalf if the satellite goes down.

\todo{future work - shared reputation, satellite reputation}

\section{Garbage collection}

When data is moved or deleted, it's important to inform impacted storage nodes
that they are no longer required to store that data. Unfortunately, sometimes
storage nodes will be temporarily unavailable and delete messages will be
missed. In these cases, data that is no longer needed is considered
{\em garbage}. Payers only pay for data they expect to be stored, so storage
nodes with lots of garbage will find less earnings than they would
otherwise be entitled to unless a garbage collection system is employed.

A garbage collection algorithm is a method for freeing no-longer used resources.
A {\em precise} garbage collector collects all garbage exactly and
leaves no additional garbage, whereas a {\em conservative} garbage collector may
leave some small proportion of garbage around given some other tradeoffs,
often with the aim of improving performance.
As long as a conservative garbage collector is used in our system, it should
be assumed that the cost of storage owed to a storage node is high enough
to amortize the cost of storing the garbage.

When data is deleted through the client, the metadata system (and thus a
satellite, with satellite reputation on the line) will require proof that
deletes were issued to a configurable minimum number of storage nodes.
This means that every time
data is deleted, storage nodes that are online and reachable will get
notification right away.

For the nodes that miss initial delete messages, we propose a conservative
garbage collection strategy. Periodically, storage nodes will request
a highly-compressible data structure such as a
{\em Bloom filter} \cite{bloom-filter} from satellites that contains hints about
what pieces a node is expected to continue storing.
A Bloom filter is a mechanism that can
answer certain set membership questions, describing whether an element
{\em isn't contained} or
{\em maybe contained}, but can not determine whether an element
{\em is contained} in the set.
Satellites will reject
requests for these data structures that happen too frequently.
By returning a data
structure tailored to each node on a periodic schedule, a satellite can give a
storage node the ability to clean up garbage data to a configurable tolerance.

Because Bloom filters are probabilistic and their collision risk is
configurable, the conservative garbage collector can be tuned to eliminate
garbage down to an acceptable tolerance, given the tradeoff of additional
bandwidth for these larger, more exact cleanup messages. Further, each time a
Bloom filter is generated, it can be generated with a new hashing seed, lowering
the probability that a specific piece of garbage consistently gets
missed by the garbage collector.

Because this garbage collection system is not precise, storage nodes have a
strong incentive to stay online to witness as many delete messages as possible.
If a storage node misses a handful of delete messages due to an outage, the
garbage will eventually get cleaned up with enough Bloom filter based cleanups.
On the other hand, because this garbage collection system is not precise,
bandwidth overhead for negotiating the list of pieces a storage node must store
will be efficient and small.

\todo{See our future work section about undeletes in
the case of bugs or mistaken file removal.}

\todo{future work: is a bloom filter the best data structure?}

\section{Uplink}

The uplink provides an S3-compatible drop-in interface for applications that
need to store data but don't want to bother with the complexities of distributed
storage directly. The uplink is a simple service layer on top of libstorj,
which is a library that provides access to storing and retrieving data in the
Storj network.

The uplink (via libstorj) first encrypts data, erasure encodes it, then
streams it out to storage nodes, all while coordinating with a chosen satellite
for metadata and tracking.

The uplink should run co-located with wherever data is generated, and will
communicate directly with storage nodes so as to avoid central bandwidth costs.

\chapter{Product details}\label{sec:product_details}

\section{Quality control and branding}
\section{Durability}

\todo{discuss quality control and branding}
\todo{insurance}

\section{Detailed walkthroughs}

\subsection{Uploads}

When a user uploads a file:

\begin{itemize}
\item First, data begins transfer to the console.
\item The console chooses an encryption key and starting nonce for
  this segment and begins encrypting incoming data with authenticated
  encryption as it flows through.
\item The console buffers data until it knows whether the incoming file is
short enough to be an inline segment or a remote segment. We'll assume a remote
segment.
\item The console sends a request to the satellite to prepare for the storage
of this first segment. The satellite will:
  \begin{itemize}
  \item Confirm that the console has appropriate authorization and funds for
    the request. The console must have an account with this
specific satellite already.
  \item Make a selection of nodes that conform to the bucket's configured
    durability, performance, geographic and reputation requirements that have
    enough resources.
  \item Return a list of nodes, along with their contact information and
    signed unrestricted bandwidth allocation messages, and a chosen root piece
    id.
  \end{itemize}
\item The console will take this information and begin parallel connections to
  all of the chosen storage nodes via the bandwidth allocation protocol.
\item The console will begin breaking the segment into stripes and then
  erasure encode each stripe.
\item The generated erasure shares will be concatenated into \x{pieces} as they
  transfer to each storage node in parallel.
\item The erasure encoding will be configured to over-encode to more pieces
  than needed. This will allow for the elimination of long tails and the
  significant improvement of visible performance by allowing the console to
  cancel the slowest uploads.
\item The data will continue to transfer until the maximum segment size is hit
  or the stream ends, whichever is sooner.
\item The storage node will store the largest restricted bandwidth allocation,
the
  TTL of the segment (if any), and the data itself by the storage node-specific
  piece
  id (the HMAC of the root piece id and the storage node's id).
\item If the upload is aborted for any reason, the storage node will keep the
  largest bandwidth allocation it received but otherwise will throw away all
  relevant request data.
\item The console encrypts the random encryption key chosen for this file
  with a deterministic hierarchical key.
\item The console will upload a \x{pointer} back to the satellite, which
  contains information on which storage nodes were
  ultimately successful, what encrypted path was chosen for this segment, which
  erasure code algorithm was used, the chosen piece id, the
  encrypted encryption key and other metadata, and a signature.
\item The console will then proceed with the next segment, continuing to
  process segments until the entire stream has completed. Each segment gets
  a new encryption key, but the nonce monotonically increases from the previous
  segment.
\item The last segment stored in the stream will contain additional metadata
  about how many segments the stream contained, how large the segments were
  in bytes, and the starting nonce of the first segment.
\item The storage nodes will later send the largest restricted
  bandwidth allocation they received as part of the upload to the appropriate
  satellite for later payment.
\end{itemize}

\subsection{Download}

When a user downloads a file:

\begin{itemize}
\item First, a request for data is received by the console.
\item The console tries to reduce round trips to the satellite
  by speculatively requesting the pointers for the first few segments, in
  addition to the pointer for the last segment from the satellite. The last
  segment is needed to learn the size of the object, how many segments
  there are, and how big the segments are.
\item For every segment pointer requested, the satellite will:
  \begin{itemize}
  \item Validate that the console has access to download the segment pointer
    and funds to pay for its downloading.
  \item Generate an unrestricted bandwidth allocation for the segment.
  \item Look up the contact information for the storage nodes listed in the
  pointer.
  \item Return the requested segment, the bandwidth allocations, and contact
    info.
  \end{itemize}
\item The console will calculate if more segments are necessary for the
  data request it received, requesting the remaining segment pointers if so.
\item Once all necessary segment pointers have been returned, if the requested
  segments are not inline, the satellite will initiate parallel requests
  via the bandwidth allocation protocol to all appropriate storage nodes for the
  appropriate erasure share ranges inside of each stored piece.
\item Because not all erasure shares are necessary for recovery, long tails
  will be eliminated and a significant and visible performance improvement will
  be gained by allowing the console to cancel the slowest downloads.
\item If the download is aborted for any reason, the storage node will keep the
  largest bandwidth allocation it received but otherwise will throw away all
  relevant request data.
\item The console will combine the retrieved erasure shares into stripes.
\item The storage nodes will later send the largest restricted
  bandwidth allocation they received as part of the download to the appropriate
  satellite for later payment.
\end{itemize}

\subsection{Delete}

When a user deletes a file:

\begin{itemize}
\item First, the delete operation is received by the console.
\item The console requests all of the segment pointers for the file.
\item For every segment pointer, the satellite will:
  \begin{itemize}
  \item Validate that the console has access to delete the segment pointer.
  \item Generate a signed agreement for the deletion of the segment, so the
    storage node knows the satellite is expecting the delete to proceed.
  \item Look up the contact information for the storage nodes listed in the
  pointer.
  \item Return the segments, the agreements, and contact info.
  \end{itemize}
\item For all of the segments that are not inline, the satellite will
  initiate parallel requests to all appropriate storage nodes to signal that the
  pieces are being removed.
\item The storage nodes will return a signed message saying the storage node
received
the
  delete operation and will delete the file and its bookkeeping info.
\item The console will upload back to the satellite all of the signed
  messages it received from working storage nodes. The satellite will require an
  adjustable percent of the total storage nodes to sign messages successfully
  to ensure that the console did its part in letting storage nodes know the
  object
  has been deleted.
\item The satellite will remove the segment pointers and stop charging and
  paying for them.
\item The console will return success.
\item Periodically, storage nodes will ask the satellite for generated garbage
  collection messages that will help storage nodes who were offline during the
  main
  deletion event.
  The garbage collection messages will assist the storage node in pruning data
  that is
  no longer live. Initially, these garbage collection messages will be tunable
  Bloom filters to allow the storage node to probabilistically prune unneeded
  data
  without using much bandwidth.
  Satellites will reject requests for garbage collection messages that
  happen too frequently.
\end{itemize}

\subsection{List}

When a user wants to receive many files:

\begin{itemize}
\item First, a request for listing objects is received by the console.
\item The console will translate the request on unencrypted paths to encrypted
  paths.
\item The console will request from the satellite the appropriate list of
  encrypted paths.
\item The satellite will validate that the console has appropriate access
  and then return the requested list.
\item The console will decrypt the return results and return them.
\end{itemize}

It's worth pointing out that because the satellite stores paths in sorted
order, the order returned to the customer is sorted by the encrypted
path element, which means that unencrypted paths will be in random but
deterministic order. If a customer wants sorted paths and doesn't mind the
satellite operator having access to unencrypted paths, the customer can opt
into unencrypted (and thus lexicographically sorted) paths.

\todo{future work section about order-preserving encryption}

\subsection{Audits}

The auditing process:

\begin{itemize}
\item Each satellite has a queue of audits, where an audit will entail
  validating a specific stripe of a segment across a set of storage nodes.
\item Periodically, satellites will choose a stripe to audit by selecting
  an object uniformly at random, weighted by the number of bytes it has,
and place that stripe into the audit queue.
\item Similarly, satellites will choose a stripe to audit by identifying
  storage nodes that have had fewer recent audits than other storage nodes, and
  selecting
  a stripe at random from the data contained by that storage node. That stripe
  audit
  request will also be placed in the audit queue.
\item Satellites will process elements from the queue.
\item For each stripe request, the satellite will perform the entire download
  operation for that small stripe range. Unlike standard downloads, the stripe
  request does not need to be performant; the satellite will attempt to
  download all of the erasure shares for the stripe and will wait for slow
  storage nodes.
\item After receiving as many shares as possible within a generous timeout,
  the erasure shares will be analyzed to discover which, if any, are wrong.
  Satellites will take note of storage nodes that return invalid data, and if
  a
  storage node returns too much invalid data, the storage node will be
  blacklisted by the
  satellite and marked as bad. The satellite will not pay the storage
  node going
  forward, nor will it select it for new data.
\end{itemize}

\subsection{Repair}

The repair process:

\begin{itemize}
\item Each satellite periodically will ping every storage node it knows
about,
  either as part of the audit process, or via standard overlay ping operations.
\item The satellite will keep track of nodes that fail to respond and mark
  them as down.
\item When a node is marked down or is marked bad via the audit process, the
  pointers that point to that storage node will be considered for repair.
  Pointers
  keep track of their minimum allowable redundancy. If a pointer is not stored
  on enough good and online storage nodes, it will be added to the repair queue.
\item A worker process will take segment pointers off the repair queue. When
  a segment pointer is taken off the repair queue, the entire segment will be
  downloaded. Unlike audits, only enough pieces for accurate repair are needed.
  Unlike streaming downloads, the repair system can wait for the entire segment
  before starting. As a result, pieces are compared against a Merkle tree of
  hashes for correctness prior to repair, where the Merkle root is stored in
  the pointer.
\item Once enough correct pieces are recovered, the missing pieces are
  regenerated.
\item The satellite selects some new nodes and uploads the new pieces to
  those new nodes via the normal upload process.
\item The satellite updates the pointer's metadata.
\end{itemize}

\subsection{Payment}

The payment process:

\begin{itemize}
\item First, a satellite will choose a roll-up period. This is a period of
  time -- defaulting to a day -- that payment for data at rest is calculated.
\item Each roll-up period, a satellite will consider all of the files it
  believes are currently stored on each storage node. Satellites will keep track
of payments owed to each storage node for each rollup period, based on
the data kept on each storage node.
\item Periodically, storage nodes will send in bandwidth allocation reports.
When a
  satellite receives these, it calculates the owed funds along with the
  outstanding data at rest calculations, and sends the funds to the storage
  node's requested destination.
\end{itemize}

\section{Reliability}

\chapter{Future Areas of Research}\label{sec:future_work}

\todo{ Storj is a work in progress, and many features are planned for future
versions. There are relatively few examples of functional distributed systems at
scale, and many areas of research are still open. }

\section{Improving user experience around metadata}

\todo{automatic exports, backups, distributed consensus}

\section{Fast Byzantine Consensus}

Over time, we plan to program the satellite out of the platform.
The satellite's role on the network means that the network could be prone
to some
centralization if others outside of the Storj Labs team do not run their own
satellites. The biggest challenge is achieving fast byzantine consensus,
where storage node nodes can interact with one another, share encoded pieces of
files,
and still operate within the performance levels users will expect from a
platform that is competing with traditional cloud storage providers.

Our team will be researching ways to store lots of small pieces of metadata
in a distributed manner, even when those pieces are constantly changing. There
currently is not a way to achieve this without significant investment in time,
compute, and bandwidth. A practical byzantine fault tolerance algorithm could
work. They are generally faster and use less disk space than blockchain
protocols, however there is significant trade off around network usage and
coordination contention, as there could be problematic overlap with two storage
nodes trying to communicate with one another at the same time.

\section{Distributed Repair}

The system can detect when a file's Reed Solomon erasure encoding pieces fall
below a certain threshold. At that time, the file must be repaired, with
the new pieces being stored on new storage nodes.
Currently, this
repair process takes place on the satellite. The satellite downloads all
the file fragments needed to repair the file, the file is rebuilt, and the
previously missing shards are sent to new storage nodes
selected by the satellite.

Long term, it would be better to create a technique where file repair takes
place in a distributed manner on storage nodes, putting their excess CPU
cycles
to work. This will be a first step to eliminating the satellite. This
approach would also be more decentralized than file repair on satellites. It
is also more efficient to execute this operation at the edge of the network.

The system would need more checks and balances to ensure the storage node is
correctly
executing a repair and that the data inside the encrypted file is accurate.
Merkle tree roots will greatly help with distributed repair. The storage
node
executing the repair would get approval from the satellite to repair a file,
the satellite would share its merkle tree root with the storage node and
notify
which storage nodes should store the restored file pieces. The storage node
would then
download the file pieces needed for the repair from the storage nodes where they
reside. The repair node would execute the repair and run the shards
through the merkle tree root to prove the data was correct and properly
repaired. We are currently taking the steps needed to ensure the network and our
data format will support merkle tree repair in the future.

\section{Order-preserving encryption}

\todo{}

\newpage \appendix

\chapter{Attacks}

As with any distributed system, a variety of attack vectors exist. Many of these
are common to all distributed systems. Some are storage-specific and will apply
to any distributed storage system.

\section{Sybil}

Sybil attacks involve the creation of large amounts of nodes in an attempt to
disrupt network operation by hijacking or dropping messages. Kademlia
is reasonably
resistant to Sybil attacks, because
it relies on message redundancy and a concrete distance metric.
A node's neighbors in the network are selected by
Node ID from an evenly distributed pool, and most messages are sent to at least
three neighbors. If a Sybil attacker controls 50\% of the network, it
successfully isolates only 12.5\% of honest nodes. While reliability and
performance will degrade, the network will still be functional unless a large
portion of the network consists of colluding Sybil nodes.

\todo{discuss vetting period}

\subsection{Honest Geppetto}

The Honest Geppetto attack is a storage-specific variant of the Google attack.
The attacker operates a large number of 'puppet' nodes on the network,
accumulating trust and contracts over time. Once a certain threshold is reached,
he pulls the strings on each puppet to execute a hostage attack with the data
involved, or simply drops each node from the network. The best defense
against this attack is to create a network of sufficient scale that this attack
is ineffective. In the meantime, this can be partially prevented by relatedness
analysis of nodes. Bayesian inference across downtime, latency, and other
attributes can be used to assess the likelihood that two nodes are operated by
the same organization, and data owners can and should attempt to distribute
shards across as many unrelated nodes as possible.

\section{Eclipse}

An eclipse attack attempts to isolate a node or set of node in the network
graph, by ensuring that all outbound connections reach malicious nodes. Eclipse
attacks can be hard to identify, as malicious nodes can be made to function
normally in most cases, only eclipsing certain important messages or
information. Storj addresses eclipse attacks by using public key hashes as Node
IDs. In order to eclipse any node in the network, the attacker must repeatedly
generate key pairs until it finds three keys whose hashes are closer to the
targeted node than its nearest non-malicious neighbor, and must defend that
position against any new nodes with closer IDs. This is, in essence, a
proof-of-work problem whose difficulty is proportional to the number of nodes in
the network.

It follows that the best way to defend against eclipse attacks is to increase
the number of nodes in the network. For large networks it becomes prohibitively
expensive to perform an eclipse attack (see Section 6.2). Furthermore, any node
that suspects it has been eclipsed may trivially generate a new keypair and node
ID, thus restarting the proof-of-work challenge.

S/Kademlia additionally assists in preventing eclipse attacks by ensuring
multiple concurrent disjoint lookup paths through the network.

\section{Hostage Bytes}

The hostage byte attack is a storage-specific attack where malicious storage
nodes
refuse to transfer shards, or portions of shards, in order to extort additional
payments from data owners. Data owners should protect themselves against hostage
byte attacks by storing shards redundantly across multiple nodes (see Section
2.7). As long as the client keeps the bounds of its erasure encoding a secret,
the malicious storage node cannot know what the last byte is. Redundant storage
is not
a complete solution for this attack, but addresses the vast majority of
practical applications of this attack. Defeating redundancy requires collusion
across multiple malicious nodes, which is difficult to execute in practice.

\chapter{Selected use cases}

\input{RS-appendix-files/model.tex}
\input{RS-appendix-files/numexp.tex}
\input{RS-appendix-files/conc.tex}

\chapter{Distributed consensus}

\section{Non-byzantine}

\subsection{Aside about distributed consensus}

A long and challenging area of research has been directed toward getting a
group of computers to agree on a set of values
\bs{what kind of values? principles? or numerical-type values?},
with the goal of constructing a
horizontally-scalable database that works in the face of expected failures
(crash failures, for example: failures where a server simply shuts down).
Fortunately, this research has led to some really exciting technology.

The biggest issue with getting a group of computers to agree is that messages
can be lost. How this impacts decision making is succinctly described by the
``Two Generals' Problem" \cite{two-generals}
\footnote{earlier described as a problem
between groups of gangsters \cite{two-gangsters}}, in which two armies try to
communicate in the face of potentially lost messages. Both armies have already
agreed to attack a shared enemy, but have yet to decide on a time. Both armies
must attack at the same time or else failure is assured. Both armies can send
messengers, but the messengers are often captured by the enemy. Both armies must
know what time to attack and that the other army has also agreed to this time.

Ultimately, a solution to the two generals' problem with a finite number of
messages is readily seen to be impossible, so engineering approaches have had
to embrace uncertainty by necessity. Many distributed systems make trade-offs to
deal with this uncertainty. Some systems embrace {\em consistency}, which means
that the system will choose downtime over inconsistent answers. Other
systems embrace {\em availability}, which means that the system chooses
potentially inconsistent answers over downtime. The widely-cited CAP
theorem \cite{cap} states that every system must choose only two of consistency,
availability, and partition tolerance. Due to the inevitability of network
failures, partition tolerance is non-negotiable, so when a partition happens,
every system must choose to sacrifice either consistency or availability. Many
systems sacrifice both (sometimes by accident).

In the CAP theorem, consistency means that every read receives the most recent
write or an error, so an inconsistent answer means the system returned something
besides the most recent write without obviously failing. More generally, there
are a number of {\em consistency models} that may be acceptable by making
various tradeoffs. Linearizability, sequential consistency, causal consistency,
PRAM consistency, eventual consistency, read-after-write consistency, etc., are
all models for discussing how a history of events appears to various
participants in a distributed system.\footnote{If differing consistency models
are new to you, it may be worth reading about them in Kyle Kingbury's excellent
tutorial \cite{aphyr-consistency}. If you're wondering why computers can't just
use the current time to order events, keep in mind it is exceedingly difficult
to get computers to even agree on that \cite{no-now}.}

Amazon S3 generally provides {\em read-after-write consistency}, though in some
cases will provide {\em eventual consistency} instead \cite{s3-consistency}.
Arguably, there may be some flexibility here which allows for the selection
of alternate consistency models that suit us better while still broadly
providing S3 compatibility.
Many distributed databases provide eventual consistency by
default, such as Dynamo \cite{dynamo} and Cassandra \cite{cassandra}.

Linearizability in a distributed system is often much more desirable
\bs{than what?}, as it is
useful as a building block for many higher level data structures and operations
such as distributed locks and other coordination techniques. Initially, early
efforts centered around two-phase commit, then three-phase commit, which both
suffered due to issues similar to the two generals' problem. Things were looking
bad in 1985 when the FLP-impossibility paper \cite{flp} proved that no algorithm
could reach linearizable consensus in bounded time. Then in 1988, Barbara Liskov
and Brian Oki published the Viewstamped Replication algorithm \cite{vr} which
was the first linearizable distributed consensus algorithm. Unaware of the VR
publication, Leslie Lamport set out to prove linearizable distributed consensus
was impossible \cite{paxos-note}, but instead in 1989 proved it was possible by
publishing his own Paxos algorithm \cite{paxos}, which for some reason became
significantly more popular. Ultimately both algorithms have a large amount in
common.

Despite Lamport's claims that Paxos is actually simple \cite{paxos-simple},
many papers have been published since then
challenging that assertion. Google's description of their attempts to implement
Paxos are described in Paxos Made Live \cite{paxos-live},
and Paxos Made Moderately
Complex \cite{paxos-complex} is an attempt to try and fill in all the details of
the protocol. The entire basis of the Raft algorithm is rooted in trying to
wrangle and simplify the complexity of Paxos \cite{raft}. Ultimately, after an
upsetting few decades, reliable implementations of Paxos, Raft, Viewstamped
Replication \cite{vrr}, Chain Replication \cite{chain-rep}, and Zab \cite{zab}
now exist, with ongoing work to improve the situation
further \cite{epaxos,paxos-flexible}. Arguably, part of Google's early success
was in spending the time to build their internal Paxos-as-a-service distributed
lock system, Chubby \cite{chubby}. Most of Google's most famous internal data
storage tools such as Bigtable \cite{bigtable} depend on Chubby for
correctness. Spanner \cite{spanner} -- perhaps one of the most incredible
distributed databases in the world -- is mainly just two-phase commit on top of
multiple Paxos groups.

Reliable distributed consensus algorithms have been game-changing for many
applications requiring fault-tolerant storage.

\section{Byzantine}

As mentioned in our design constraints, we expect most nodes to be {\em
rational} and some to be {\em byzantine}, but few-to-none to be {\em
altruistic}. Unfortunately, all of the previous algorithms we discussed assume a
collection of altruistic nodes.

There have been a number of attempts to solve the Byzantine fault tolerant
distributed consensus problem
\cite{bitcoin,pbft,qu,fab,fab-revisited,zyzzyva,rbft,
tangaroa,tendermint,aliph,hashgraph,honeybadger,algorand,casper,
tangle,avalanche,parsec,mickens-bft}. Each of these algorithms make some
additional tradeoffs that non-Byzantine distributed consensus algorithms don't
require to deal with the potential for uncooperative nodes. For example,
PBFT \cite{pbft} causes a significant amount of network overhead. Bitcoin
\cite{bitcoin} intentionally limits the transaction rate with changing
proof-of-work difficulty, in addition to requiring all participants to keep a
full copy of all change histories (like other blockchain-based
solutions).

%(PBFT \cite{pbft} (Barbara Liskov again with the
%first solution out of the gate), Q/U \cite{qu}, FaB \cite{fab} (but see
%\cite{fab-revisited}), Bitcoin \cite{bitcoin}, Zyzzyva \cite{zyzzyva} (but also
%see \cite{fab-revisited}), RBFT \cite{rbft}, Tangaroa \cite{tangaroa},
%Tendermint \cite{tendermint}, Aliph \cite{aliph}, Hashgraph \cite{hashgraph},
%HoneybadgerBFT\cite{honeybadger}, Algorand\cite{algorand}, Casper\cite{casper},
%Tangle\cite{tangle}, Avalanche\cite{avalanche}, PARSEC\cite{parsec}, and
%others\cite{mickens-bft}).

\todo{talk about merkle-dag, git-inspired approaches to metadata, and how
they struggle with conflict resolution due to the lack of crdt-like options for
file systems}

\newpage
\bibliographystyle{unsrt}
\begingroup
\raggedright
\bibliography{biblio}
\endgroup

\end{document}
